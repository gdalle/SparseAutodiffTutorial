@misc{abadiTensorFlowLargescaleMachine2015,
  title = {TensorFlow: Large-scale machine learning on heterogeneous systems},
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  url = {https://www.tensorflow.org/},
  keywords = {tracer}
}

@book{andersonLAPACKUsersGuide1999,
  title = {LAPACK users' guide},
  author = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, S. and Demmel, J. and Dongarra, J. and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
  year = {1999},
  edition = {3},
  publisher = {Society for Industrial and Applied Mathematics},
  address = {Philadelphia, PA},
  isbn = {0-89871-447-8 (paperback)},
  keywords = {tracer}
}

@article{anderssonCasADiSoftwareFramework2019,
  title = {CasADi: a software framework for nonlinear optimization and optimal control},
  shorttitle = {CasADi},
  author = {Andersson, Joel A. E. and Gillis, Joris and Horn, Greg and Rawlings, James B. and Diehl, Moritz},
  year = {2019},
  month = mar,
  journal = {Mathematical Programming Computation},
  volume = {11},
  number = {1},
  pages = {1--36},
  issn = {1867-2957},
  doi = {10.1007/s12532-018-0139-4},
  url = {https://doi.org/10.1007/s12532-018-0139-4},
  urldate = {2024-05-07},
  abstract = {We present CasADi, an open-source software framework for numerical optimization. CasADi is a general-purpose tool that can be used to model and solve optimization problems with a large degree of flexibility, larger than what is associated with popular algebraic modeling languages such as AMPL, GAMS, JuMP or Pyomo. Of special interest are problems constrained by differential equations, i.e. optimal control problems. CasADi is written in self-contained C++, but is most conveniently used via full-featured interfaces to Python, MATLAB or Octave. Since its inception in late 2009, it has been used successfully for academic teaching as well as in applications from multiple fields, including process control, robotics and aerospace. This article gives an up-to-date and accessible introduction to the CasADi framework, which has undergone numerous design improvements over the last 7~years.},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/FCTXD6ZC/Andersson et al. - 2019 - CasADi a software framework for nonlinear optimiz.pdf}
}

@article{aslanKinematicApplicationsHyperDual2021,
  title = {Kinematic Applications of Hyper-Dual Numbers},
  author = {Aslan, Selahattin},
  year = {2021},
  month = oct,
  journal = {International Electronic Journal of Geometry},
  volume = {14},
  number = {2},
  pages = {292--304},
  publisher = {Kazım İLARSLAN},
  issn = {1307-5624},
  doi = {10.36890/iejg.888373},
  url = {https://dergipark.org.tr/en/pub/iejg/issue/65263/888373},
  urldate = {2024-05-08},
  abstract = {Hyper-dual numbers are a new number system that is an extension of dual numbers. A hyper-dual number can be written uniquely as an ordered pair of dual numbers. In this paper, some basic algebraic properties of hyper-dual numbers are given using their ordered pair representaions of dual numbers. Moreover, the geometric interpretation of a unit hyper-dual vector is given in module as a dual line. And a geometric interpretation of a subset of unit hyper-dual sphere (the set of all unit hyper-dual vectors) is given as two intersecting perpendicular lines in 3-dimensional real vector space.},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/JUY68Y5T/Aslan - 2021 - Kinematic Applications of Hyper-Dual Numbers.pdf}
}

@article{bartholomew-biggsOptimizationAutomaticDifferentiation1994,
  title = {Optimization \& automatic differentiation in Ada: some practical experience},
  shorttitle = {Optimization \& automatic differentiation in Ada},
  author = {{Bartholomew-Biggs}, M.C. and {Bartholomew-Biggs}, L. and Christianson, B.},
  year = {1994},
  month = jan,
  journal = {Optimization Methods and Software},
  volume = {4},
  number = {1},
  pages = {47--73},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556789408805577},
  url = {https://doi.org/10.1080/10556789408805577},
  urldate = {2024-05-08},
  abstract = {This paper describes an investigation into the performance of three Ada packages for automatic differentiation. Two of these implement the forward accumulation approach while the third employs reverse accumulation. Each package is used to provide gradient information required by a number of optimization calculations, including examples of unconstrained, constrained and least-squares problems. The results show how automatic differentiation methods can be influenced in practice by the size, complexity and sparsity of a problem. They also demonstrate ways in which the methods should interface with different types of optimization procedure. Finally, and perhaps most significantly, the results show how the performance of automatic differentiation codes can depend on hardware and system software considerations that are sometimes ignored by numerical mathematicians. A subsidiary aim of this paper is to provide a “shop window” for user friendly forms of automatic differentiation. The underlying mathematical ideas have been quite widely discussed in the literature: but their implementation and use seems to have been perceived as too difficult for the non-specialist. The examples in this paper are intended to demonstrate that this need not be the case},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/E8FPETQI/Bartholomew-Biggs et al. - 1994 - Optimization & automatic differentiation in Ada s.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/IQTCUC4D/bartholomew-biggs1994.pdf.pdf}
}

@article{baurComplexityPartialDerivatives1983,
  title = {The complexity of partial derivatives},
  author = {Baur, Walter and Strassen, Volker},
  year = {1983},
  month = feb,
  journal = {Theoretical Computer Science},
  volume = {22},
  number = {3},
  pages = {317--330},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(83)90110-X},
  url = {https://www.sciencedirect.com/science/article/pii/030439758390110X},
  urldate = {2024-05-07},
  abstract = {Let L denote the nonscalar complexity in k(x1,…, xn). We prove L(ƒ,∂ƒ/∂x1,…,∂ƒ/∂xn)⩽3L(ƒ). Using this we determine the complexity of single power sums, single elementary symmetric functions, the resultant and the discriminant as root functions, up to order of magnitude. Also we linearly reduce matrix inversion to computing the determinant.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/YKEAGMGI/030439758390110X.html}
}

@article{baydinAutomaticDifferentiationMachine2018,
  title = {Automatic Differentiation in Machine Learning: a Survey},
  shorttitle = {Automatic Differentiation in Machine Learning},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {153},
  pages = {1--43},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v18/17-468.html},
  urldate = {2023-03-12},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply âautodiffâ, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names âdynamic computational graphsâ and âdifferentiable programmingâ. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms âautodiffâ, âautomatic differentiationâ, and âsymbolic differentiationâ as these are encountered more and more in machine learning settings.},
  keywords = {autodiff,done,inferopt,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/MP87JETA/Baydin et al_2018_Automatic Differentiation in Machine Learning.pdf}
}

@misc{bellComputingSparseJacobians2021,
  title = {Computing Sparse Jacobians and Hessians Using Algorithmic Differentiation},
  author = {Bell, Bradley M. and Kristensen, Kasper},
  year = {2021},
  month = nov,
  number = {arXiv:2111.05207},
  eprint = {2111.05207},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.05207},
  url = {http://arxiv.org/abs/2111.05207},
  urldate = {2024-05-07},
  abstract = {Stochastic scientific models and machine learning optimization estimators have a large number of variables; hence computing large sparse Jacobians and Hessians is important. Algorithmic differentiation (AD) greatly reduces the programming effort required to obtain the sparsity patterns and values for these matrices. We present forward, reverse, and subgraph methods for computing sparse Jacobians and Hessians. Special attention is given the the subgraph method because it is new. The coloring and compression steps are not necessary when computing sparse Jacobians and Hessians using subgraphs. Complexity analysis shows that for some problems the subgraph method is expected to be much faster. We compare C++ operator overloading implementations of the methods in the ADOL-C and CppAD software packages using some of the MINPACK-2 test problems. The experiments are set up in a way that makes them easy to run on different hardware, different systems, different compilers, other test problem and other AD packages. The setup time is the time to record the graph, compute sparsity, coloring, compression, and optimization of the graph. If the setup is necessary for each evaluation, the subgraph implementation has similar run times for sparse Jacobians and faster run times for sparse Hessians.},
  archiveprefix = {arXiv},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/SMWSXP2Y/Bell and Kristensen - 2021 - Computing Sparse Jacobians and Hessians Using Algo.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/XE4KDQ6I/2111.html}
}

@misc{betancourtConceptualIntroductionHamiltonian2018,
  title = {A Conceptual Introduction to Hamiltonian Monte Carlo},
  author = {Betancourt, Michael},
  year = {2018},
  month = jul,
  number = {arXiv:1701.02434},
  eprint = {1701.02434},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.02434},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2024-01-19},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/THGNR4XM/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/8DAX42D4/1701.html}
}

@inproceedings{bettencourtTaylorModeAutomaticDifferentiation2019,
  title = {Taylor-Mode Automatic Differentiation for Higher-Order Derivatives in JAX},
  booktitle = {Program Transformations for ML Workshop at NeurIPS 2019},
  author = {Bettencourt, Jesse and Johnson, Matthew J. and Duvenaud, David},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=SkxEF3FNPH},
  urldate = {2024-05-06},
  abstract = {One way to achieve higher-order automatic differentiation (AD) is to implement first-order AD and apply it repeatedly. This nested approach works, but can result in combinatorial amounts of redundant work. This paper describes a more efficient method, already known but with a new presentation, and its implementation in JAX. We also study its application to neural ordinary differential equations, and in particular discuss some additional algorithmic improvements for higher-order AD of differential equations.},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/VAFIELQF/Bettencourt et al_2019_Taylor-Mode Automatic Differentiation for Higher-Order Derivatives in JAX.pdf}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: A Fresh Approach to Numerical Computing},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  pages = {65--98},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/141000671},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  urldate = {2022-12-03},
  langid = {english},
  keywords = {bootstrap,gnn,hmm,inferopt,povar,thesis,tracer,viva},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/YWLISSFK/Bezanson et al_2017_Julia.pdf}
}

@article{bischofADIFORGeneratingDerivative1992,
  title = {ADIFOR–Generating Derivative Codes from Fortran Programs},
  author = {Bischof, Christian and Carle, Alan and Corliss, George and Griewank, Andreas and Hovland, Paul},
  year = {1992},
  journal = {Scientific Programming},
  volume = {1},
  number = {1},
  pages = {717832},
  issn = {1875-919X},
  doi = {10.1155/1992/717832},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/1992/717832},
  urldate = {2024-11-05},
  abstract = {The numerical methods employed in the solution of many scientific computing problems require the computation of derivatives of a function f Rn→Rm. Both the accuracy and the computational requirements of the derivative computation are usually of critical importance for the robustness and speed of the numerical solution. Automatic Differentiation of FORtran (ADIFOR) is a source transformation tool that accepts Fortran 77 code for the computation of a function and writes portable Fortran 77 code for the computation of the derivatives. In contrast to previous approaches, ADIFOR views automatic differentiation as a source transformation problem. ADIFOR employs the data analysis capabilities of the ParaScope Parallel Programming Environment, which enable us to handle arbitrary Fortran 77 codes and to exploit the computational context in the computation of derivatives. Experimental results show that ADIFOR can handle real-life codes and that ADIFOR-generated codes are competitive with divided-difference approximations of derivatives. In addition, studies suggest that the source transformation approach to automatic differentiation may improve the time to compute derivatives by orders of magnitude.},
  copyright = {Copyright © 1992 Hindawi Publishing Corporation.},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/ZZKIP96F/Bischof et al. - 1992 - ADIFOR–Generating Derivative Codes from Fortran Programs.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/Z2WYS5I8/717832.html}
}

@article{bischofAutomaticDifferentiationMATLAB2003,
  title = {Automatic Differentiation for MATLAB Programs},
  author = {Bischof, Christian and Lang, Bruno and Vehreschild, Andre},
  year = {2003},
  journal = {PAMM},
  volume = {2},
  number = {1},
  pages = {50--53},
  issn = {1617-7061},
  doi = {10.1002/pamm.200310013},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pamm.200310013},
  urldate = {2024-05-07},
  abstract = {Derivative information is required in numerous applications, including sensitivity analysis and numerical optimization. For simple functions, symbolic differentiation–done either manually or with a computer algebra system–can provide the derivatives, whereas divided differences (DD) have been used traditionally for functions defined by (potentially very complex) computer programs, even if only approximate values can be obtained this way. An alternative approach for such functions is automatic differentiation (AD), yielding exact derivatives at often lower cost than DD, and without restrictions on the program complexity. In this paper we compare the functionality and describe the use of ADMIT/ADMAT and ADiMat. These two AD tools provide derivatives for programs written in the MATLAB language, which is widely used for prototype and production software in scientific and engineering applications. While ADMIT/ADMAT implements a pure operator overloading approach of AD, ADiMat also employes source transformation techniques.},
  copyright = {Copyright © 2003 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/SJXVIUR9/Bischof et al. - 2003 - Automatic Differentiation for MATLAB Programs.pdf}
}

@article{bischofEfficientComputationGradients1996,
  title = {Efficient computation of gradients and Jacobians by dynamic exploitation of sparsity in automatic differentiation},
  author = {Bischof, Christian H. and Khademi, Peyvand M. and Buaricha, Ali and Alan, Carle},
  year = {1996},
  month = jan,
  journal = {Optimization Methods and Software},
  volume = {7},
  number = {1},
  pages = {1--39},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556789608805642},
  url = {https://doi.org/10.1080/10556789608805642},
  urldate = {2024-03-28},
  abstract = {Automatic differentiation (AD) is a technique that augments computer codes with statements for the computation of derivatives. The computational workhorse of AD-generated codes for first-order derivatives is the linear combination of vectors. For many large-scale problems, the vectors involved in this operation are inherently sparse. If the underlying function is a partially separable one (e.g., if its Hessian is sparse), many of the intermediate gradient vectors computed by AD will also be sparse, even though the final gradient is likely to be dense. For large Jacobians computations, every intermediate derivative vector is usually at least as sparse as the least sparse row of the final Jacobian. In this paper, we show that dynamic exploitation of the sparsity inherent in derivative computation can result in dramatic gains in runtime and memory savings. For a set of gradient problems exhibiting implicit sparsity, we report on the runtime and memory requirements of computing the gradients with the ADIFOR (Automatic Differentiation of FORtran) tool, both with and without employing the SparsLinC (Sparse Linear Combinations) library, and show that SparsLinC can reduce runtime and memory costs by orders of magnitude. We also compute sparse Jacobians using the SparsLinC-based approach — in the process, automatically detecting the sparsity structure of the Jacobian — and show that these Jacobian results compare favorably with those of previous techniques that require a priori knowledge of the sparsity structure of the Jacobian},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/2C6PGEL4/Bischof et al. - 1996 - Efficient computation of gradients and Jacobians b.pdf}
}

@article{blackfordUpdatedSetBasic2002,
  title = {An updated set of basic linear algebra subprograms (BLAS)},
  author = {Blackford, L. Susan and Petitet, Antoine and Pozo, Roldan and Remington, Karin and Whaley, R. Clint and Demmel, James and Dongarra, Jack and Duff, Iain and Hammarling, Sven and Henry, Greg},
  year = {2002},
  month = jun,
  journal = {ACM Transactions on Mathematical Software},
  volume = {28},
  number = {2},
  pages = {135--151},
  issn = {0098-3500},
  doi = {10.1145/567806.567807},
  urldate = {2024-05-08},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/RNF7KLNS/Blackford et al. - 2002 - An updated set of basic linear algebra subprograms.pdf}
}

@inproceedings{blondelEfficientModularImplicit2022,
  title = {Efficient and Modular Implicit Differentiation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and {Llinares-López}, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
  year = {2022},
  month = oct,
  url = {https://openreview.net/forum?id=Q-HOv_zn6G},
  urldate = {2023-03-12},
  abstract = {Automatic differentiation (autodiff) has revolutionized machine learning. It allows to express complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization layers, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, so far, implicit differentiation remained difficult to use for practitioners, as it often required case-by-case tedious mathematical derivations and implementations. In this paper, we propose automatic implicit differentiation, an efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function \$F\$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of \$F\$ and the implicit function theorem to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many existing implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.},
  langid = {english},
  keywords = {autodiff,done,inferopt,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/5MNWEQPU/Blondel et al. - 2022 - Efficient and Modular Implicit Differentiation.pdf}
}

@misc{blondelElementsDifferentiableProgramming2024,
  title = {The Elements of Differentiable Programming},
  author = {Blondel, Mathieu and Roulet, Vincent},
  year = {2024},
  month = jul,
  number = {arXiv:2403.14606},
  eprint = {2403.14606},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.14606},
  url = {http://arxiv.org/abs/2403.14606},
  urldate = {2024-08-06},
  abstract = {Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.},
  archiveprefix = {arXiv},
  keywords = {autodiff,done,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/6LFEKUZA/Blondel_Roulet_2024_The Elements of Differentiable Programming.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/7VQWVTKF/2403.html}
}

@inproceedings{bolteComplexityNonsmoothAutomatic2023,
  title = {On the complexity of nonsmooth automatic differentiation},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Bolte, Jerome and Boustany, Ryan and Pauwels, Edouard and {Pesquet-Popescu}, Béatrice},
  year = {2023},
  month = feb,
  url = {https://openreview.net/forum?id=uqg3FhRZaq},
  urldate = {2023-05-14},
  abstract = {Using the notion of conservative gradient, we provide a simple model to estimate the computational costs of the backward and forward modes of algorithmic differentiation for a wide class of nonsmooth programs. The complexity overhead of the backward mode turns out to be independent of the dimension when using programs with locally Lipschitz semi-algebraic or definable elementary functions. This extends considerably the Baur-Strassen's smooth cheap gradient principle. We illustrate our results by establishing fast backpropagation results of conservative gradients through feedforward neural networks with standard activation and loss functions. Nonsmooth backpropagation's cheapness contrasts with concurrent forward approaches, which have, to this day, dimensional-dependent worst case overhead estimates. We provide further results suggesting the superiority of backward propagation of conservative gradients. Indeed, we relate the complexity of computing a large number of directional derivatives to that of matrix multiplication, and we show that finding two subgradients in the Clarke subdifferential of a function is a NP-hard problem.},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/M3HZRWAU/Bolte et al. - 2023 - On the complexity of nonsmooth automatic different.pdf}
}

@article{bolteConservativeSetValued2021,
  title = {Conservative set valued fields, automatic differentiation, stochastic gradient methods and deep learning},
  author = {Bolte, Jérôme and Pauwels, Edouard},
  year = {2021},
  month = jul,
  journal = {Mathematical Programming},
  volume = {188},
  number = {1},
  pages = {19--51},
  issn = {1436-4646},
  doi = {10.1007/s10107-020-01501-5},
  url = {https://doi.org/10.1007/s10107-020-01501-5},
  urldate = {2024-01-19},
  abstract = {Modern problems in AI or in numerical analysis require nonsmooth approaches with a flexible calculus. We introduce generalized derivatives called conservative fields for which we develop a calculus and provide representation formulas. Functions having a conservative field are called path differentiable: convex, concave, Clarke regular and any semialgebraic Lipschitz continuous functions are path differentiable. Using Whitney stratification techniques for semialgebraic and definable sets, our model provides variational formulas for nonsmooth automatic differentiation oracles, as for instance the famous backpropagation algorithm in deep learning. Our differential model is applied to establish the convergence in values of nonsmooth stochastic gradient methods as they are implemented in practice.},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/NCDBS6ER/Bolte and Pauwels - 2021 - Conservative set valued fields, automatic differen.pdf}
}

@inproceedings{bolteMathematicalModelAutomatic2020,
  title = {A mathematical model for automatic differentiation in machine learning},
  booktitle = {Conference on Neural Information Processing Systems},
  author = {Bolte, Jerome and Pauwels, Edouard},
  year = {2020},
  month = dec,
  address = {Vancouver, Canada},
  url = {https://hal.archives-ouvertes.fr/hal-02734446},
  urldate = {2021-08-18},
  abstract = {Automatic differentiation, as implemented today, does not have a simple mathematical model adapted to the needs of modern machine learning. In this work we articulate the relationships between differentiation of programs as implemented in practice and differentiation of nonsmooth functions. To this end we provide a simple class of functions, a nonsmooth calculus, and show how they apply to stochastic approximation methods. We also evidence the issue of artificial critical points created by algorithmic differentiation and show how usual methods avoid these points with probability one.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/VTGLTLYS/Bolte_Pauwels_2020_A mathematical model for automatic differentiation in machine learning.pdf}
}

@article{bozdagDistributedMemoryParallelAlgorithms2010,
  title = {Distributed-Memory Parallel Algorithms for Distance-2 Coloring and Related Problems in Derivative Computation},
  author = {Bozdağ, Doruk and Çatalyürek, Ümi˙t V. and Gebremedhin, Assefaw H. and Manne, Fredrik and Boman, Erik G. and Özgüner, Füsun},
  year = {2010},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {32},
  number = {4},
  pages = {2418--2446},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {1064-8275},
  doi = {10.1137/080732158},
  url = {https://epubs.siam.org/doi/10.1137/080732158},
  urldate = {2024-06-12},
  abstract = {Acyclic and star coloring problems are specialized vertex coloring problems that arise in the efficient computation of Hessians using automatic differentiation or finite differencing, when both sparsity and symmetry are exploited. We present an algorithmic paradigm for finding heuristic solutions for these two NP‐hard problems. The underlying common technique is the exploitation of the structure of two‐colored induced subgraphs. For a graph G on n vertices and m edges, the time complexity of our star coloring algorithm is \$O(n\textbackslash overline\{d\}\_2)\$, where \$\textbackslash overline\{d\}\_k\$, a generalization of vertex degree, denotes the average number of distinct paths of length at most k edges starting at a vertex in G. The time complexity of our acyclic coloring algorithm is larger by a multiplicative factor involving the inverse of Ackermann’s function. The space complexity of both algorithms is \$O(m)\$. To the best of our knowledge, our work is the first practical algorithm for the acyclic coloring problem. For the star coloring problem, our algorithm uses fewer colors and is considerably faster than a previously known \$O(n\textbackslash overline\{d\}\_3)\$‐time algorithm. Computational results from experiments on various large‐size test graphs demonstrate that the algorithms are fast and produce highly effective solutions. The use of these algorithms in Hessian computation is expected to reduce overall runtime drastically.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/XVQCXVRQ/Bozdağ et al. - 2010 - Distributed-Memory Parallel Algorithms for Distanc.pdf}
}

@misc{bradburyJAXComposableTransformations2018,
  title = {JAX: composable transformations of Python+NumPy programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
  year = {2018},
  url = {http://github.com/google/jax},
  keywords = {tracer}
}

@article{braunSparseHessianFDPackageEstimating2017,
  title = {{\textbf{sparseHessianFD}} : An {\emph{R}} Package for Estimating Sparse Hessian Matrices},
  shorttitle = {{\textbf{sparseHessianFD}}},
  author = {Braun, Michael},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {82},
  number = {10},
  issn = {1548-7660},
  doi = {10.18637/jss.v082.i10},
  url = {http://www.jstatsoft.org/v82/i10/},
  urldate = {2024-05-08},
  abstract = {Sparse Hessian matrices occur often in statistics, and their fast and accurate estimation can improve efficiency of numerical optimization and sampling algorithms. By exploiting the known sparsity pattern of a Hessian, methods in the sparseHessianFD package require many fewer function or gradient evaluations than would be required if the Hessian were treated as dense. The package implements established graph coloring and linear substitution algorithms that were previously unavailable to R users, and is most useful when other numerical, symbolic or algorithmic methods are impractical, inefficient or unavailable.},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/VE2APZDY/Braun - 2017 - sparseHessianFD  An R Package for E.pdf}
}

@article{carterEfficientDetectionHessian2017,
  title = {Efficient detection of hessian matrix sparsity pattern},
  author = {Carter, Richard G. and Hossain, Shahadat and Sultana, Marzia},
  year = {2017},
  month = feb,
  journal = {ACM Communications in Computer Algebra},
  volume = {50},
  number = {4},
  pages = {151--154},
  issn = {1932-2240},
  doi = {10.1145/3055282.3055287},
  url = {https://dl.acm.org/doi/10.1145/3055282.3055287},
  urldate = {2024-05-07},
  abstract = {Evaluation of the Hessian matrix of a scalar function is a subproblem in many numerical optimization algorithms. For large-scale problems often the Hessian matrix is sparse and structured, and it is preferable to exploit such information when available. Using symmetry in the second derivative values of the components it is possible to detect the sparsity pattern of the Hessian via products of the Hessian matrix with specially chosen direction vectors. We use graph coloring methods and employ efficient sparse data structures to implement the sparsity pattern detection algorithms. Results from preliminary numerical testings are highly promising.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/ZKVPZP63/Carter et al. - 2017 - Efficient detection of hessian matrix sparsity pat.pdf}
}

@book{colemanAutomaticDifferentiationMATLAB2016,
  title = {Automatic Differentiation in MATLAB Using ADMAT with Applications},
  author = {Coleman, Thomas F. and Xu, Wei},
  year = {2016},
  month = jun,
  publisher = {SIAM},
  abstract = {The calculation of partial derivatives is a fundamental need in scientific computing. Automatic differentiation (AD) can be applied straightforwardly to obtain all necessary partial derivatives (usually first and, possibly, second derivatives) regardless of a code?s complexity. However, the space and time efficiency of AD can be dramatically improved?sometimes transforming a problem from intractable to highly feasible?if inherent problem structure is used to apply AD in a judicious manner.Automatic Differentiation in MATLAB using ADMAT with Applications discusses the efficient use of AD to solve real problems, especially multidimensional zero-finding and optimization, in the MATLAB environment. This book is concerned with the determination of the first and second derivatives in the context of solving scientific computing problems with an emphasis on optimization and solutions to nonlinear systems. The authors focus on the application rather than the implementation of AD, solve real nonlinear problems with high performance by exploiting the problem structure in the application of AD, and provide many easy to understand applications, examples, and MATLAB templates.},
  googlebooks = {GlqBDAAAQBAJ},
  isbn = {978-1-61197-435-5},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/EN3RAWZJ/Coleman and Xu - 2016 - Automatic Differentiation in MATLAB Using ADMAT wi.pdf}
}

@article{colemanCyclicColoringProblem1986,
  title = {The Cyclic Coloring Problem and Estimation of Sparse Hessian Matrices},
  author = {Coleman, Thomas F. and Cai, Jin-Yi},
  year = {1986},
  month = apr,
  journal = {SIAM Journal on Algebraic Discrete Methods},
  volume = {7},
  number = {2},
  pages = {221--235},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0196-5212},
  doi = {10.1137/0607026},
  url = {https://epubs.siam.org/doi/abs/10.1137/0607026},
  urldate = {2024-10-06},
  abstract = {This paper is concerned with the efficient computation of sparse Jacobian matrices of nonlinear vector maps using automatic differentiation (AD). Specifically, we propose the use of a graph coloring technique, bicoloring, to exploit the sparsity of the Jacobian matrix J and thereby allow for the efficient determination of J using AD software. We analyze both a direct scheme and a substitution process. We discuss the results of numerical experiments indicating significant practical potential of this approach.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/GSQ59ER7/Coleman_Cai_1986_The Cyclic Coloring Problem and Estimation of Sparse Hessian Matrices.pdf}
}

@article{colemanEfficientComputationSparse1998,
  title = {The Efficient Computation of Sparse Jacobian Matrices Using Automatic Differentiation},
  author = {Coleman, Thomas F. and Verma, Arun},
  year = {1998},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {19},
  number = {4},
  pages = {1210--1233},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {1064-8275},
  doi = {10.1137/S1064827595295349},
  url = {https://epubs.siam.org/doi/abs/10.1137/S1064827595295349},
  urldate = {2024-05-07},
  abstract = {The computation of large sparse Jacobian matrices is required in many important large-scale scientific problems. Three approaches to computing such matrices are considered: hand-coding, difference approximations, and automatic differentiation using the ADIFOR (automatic differentiation in Fortran) tool. The authors compare the numerical reliability and computational efficiency of these approaches on applications from the MINPACK-2 test problem collection. The conclusion is that ADIFOR is the method of choice, leading to results that are as accurate as hand-coded derivatives, while at the same time outperforming difference approximations in both accuracy and speed.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/8CUCBYGA/Coleman and Verma - 1998 - The Efficient Computation of Sparse Jacobian Matri.pdf}
}

@article{colemanEstimationSparseHessian1984,
  title = {Estimation of sparse hessian matrices and graph coloring problems},
  author = {Coleman, Thomas F. and Moré, Jorge J.},
  year = {1984},
  month = oct,
  journal = {Mathematical Programming},
  volume = {28},
  number = {3},
  pages = {243--270},
  issn = {1436-4646},
  doi = {10.1007/BF02612334},
  url = {https://doi.org/10.1007/BF02612334},
  urldate = {2024-05-07},
  abstract = {Large scale optimization problems often require an approximation to the Hessian matrix. If the Hessian matrix is sparse then estimation by differences of gradients is attractive because the number of required differences is usually small compared to the dimension of the problem. The problem of estimating Hessian matrices by differences can be phrased as follows: Given the sparsity structure of a symmetric matrixA, obtain vectorsd1,d2, …dp such thatAd1,Ad2, …Adp determineA uniquely withp as small as possible. We approach this problem from a graph theoretic point of view and show that both direct and indirect approaches to this problem have a natural graph coloring interpretation. The complexity of the problem is analyzed and efficient practical heuristic procedures are developed. Numerical results illustrate the differences between the various approaches.},
  langid = {english},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/BKQV52ET/Coleman and Moré - 1984 - Estimation of sparse hessian matrices and graph coloring problems.pdf}
}

@article{colemanSoftwareEstimatingSparse1984,
  title = {Software for estimating sparse Jacobian matrices},
  author = {Coleman, Thomas F. and Garbow, Burton S. and More, Jorge J.},
  year = {1984},
  month = aug,
  journal = {ACM Transactions on Mathematical Software},
  volume = {10},
  number = {3},
  pages = {329--345},
  issn = {0098-3500},
  doi = {10.1145/1271.1610},
  url = {https://dl.acm.org/doi/10.1145/1271.1610},
  urldate = {2024-05-17},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/IEHW6H7P/Coleman et al. - 1984 - Software for estimating sparse Jacobian matrices.pdf}
}

@article{colemanSoftwareEstimatingSparse1985,
  title = {Software for estimating sparse Hessian matrices},
  author = {Coleman, Thomas F. and Garbow, Burton S. and Moré, Jorge J.},
  year = {1985},
  month = dec,
  journal = {ACM Transactions on Mathematical Software},
  volume = {11},
  number = {4},
  pages = {363--377},
  issn = {0098-3500},
  doi = {10.1145/6187.6190},
  url = {https://dl.acm.org/doi/10.1145/6187.6190},
  urldate = {2024-05-17},
  abstract = {The solution of a nonlinear optimization problem often requires an estimate of the Hessian matrix for a function f. In large scale problems, the Hessian matrix is usually sparse, and then estimation by differences of gradients is attractive because the number of differences can be small compared to the dimension of the problem. In this paper we describe a set of subroutines whose purpose is to estimate the Hessian matrix with the least possible number of gradient evaluations.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/IIFSMQ8T/Coleman et al. - 1985 - Software for estimating sparse Hessian matrices.pdf}
}

@article{curtisEstimationSparseJacobian1974,
  title = {On the Estimation of Sparse Jacobian Matrices},
  author = {Curtis, A. R. and Powell, M. J. D. and Reid, J. K.},
  year = {1974},
  month = feb,
  journal = {IMA Journal of Applied Mathematics},
  volume = {13},
  number = {1},
  pages = {117--119},
  issn = {0272-4960},
  doi = {10.1093/imamat/13.1.117},
  url = {https://doi.org/10.1093/imamat/13.1.117},
  urldate = {2024-05-08},
  abstract = {We show how to use known constant elements in a Jacobian matrix to reduce the work required to estimate the remaining elements by finite differences.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/KKIQT6J4/curtis1974.pdf.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/VRL7WZAH/CURTIS et al. - 1974 - On the Estimation of Sparse Jacobian Matrices.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/NBQFP9I8/721987.html}
}

@inproceedings{dagreouHowComputeHessianvector2024,
  title = {How to compute Hessian-vector products?},
  booktitle = {The Third Blogpost Track at ICLR 2024},
  author = {Dagréou, Mathieu and Ablin, Pierre and Vaiter, Samuel and Moreau, Thomas},
  year = {2024},
  month = feb,
  url = {https://openreview.net/forum?id=rTgjQtGP3O},
  urldate = {2024-03-16},
  abstract = {The products between the Hessian of a function and a vector, so-called Hessian-vector product (HVPs) is a quantity that appears in optimization and machine learning. However, the computation of HVPs is often considered prohibitive, preventing practitioners from using algorithms that rely on these quantities. Standard automatic differentiation theory predicts that computing a HVP has a cost of the same order of magnitude as computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, Jax and Pytorch, allow for efficient computation of these HVPs in standard deep learning cost functions.},
  langid = {english},
  keywords = {done,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/FE3QWTK5/forum.html}
}

@misc{dalleGdalleSparseMatrixColoringsjlV04102024,
  title = {gdalle/SparseMatrixColorings.jl: v0.4.10},
  shorttitle = {gdalle/SparseMatrixColorings.jl},
  author = {Dalle, Guillaume and Montoison, Alexis and Hill, Adrian},
  year = {2024},
  month = nov,
  doi = {10.5281/zenodo.14077901},
  url = {https://zenodo.org/records/14077901},
  urldate = {2024-11-21},
  abstract = {SparseMatrixColorings v0.4.10 Diff since v0.4.9 Merged pull requests: Add compressed visualization (\#157) (@gdalle) Closed issues: Bicoloring (\#23)},
  howpublished = {Zenodo},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/4C37IAZB/14077901.html}
}

@misc{dalleJuliaDiffDifferentiationInterfacejlDifferentiationInterfacev06232024,
  title = {JuliaDiff/DifferentiationInterface.jl: DifferentiationInterface-v0.6.23},
  shorttitle = {JuliaDiff/DifferentiationInterface.jl},
  author = {Dalle, Guillaume and Hill, Adrian and Tebbutt, Will and Marcotte, Alain and Scheidegger, Andreas and ExpandingMan and Wechsler, Felix and Gerlero, Gabriel and Schmitz, Niklas and Yong, Penelope and Dixit, Vaibhav Kumar},
  year = {2024},
  month = nov,
  doi = {10.5281/zenodo.14191253},
  url = {https://zenodo.org/records/14191253},
  urldate = {2024-11-21},
  abstract = {DifferentiationInterface DifferentiationInterface-v0.6.23 Diff since DifferentiationInterface-v0.6.22 Merged pull requests: perf: check mutability of array before preallocating dual buffer (\#619) (@gdalle) CompatHelper: bump compat for JLArrays in [weakdeps] to 0.2 for package DifferentiationInterfaceTest, (keep existing compat) (\#621) (@github-actions[bot]) refactor!: remove randomness in scenario creation (\#623) (@gdalle) Fix tutorial typo (\#625) (@penelopeysm) chore: Bump codecov/codecov-action from 4 to 5 (\#626) (@dependabot[bot]) test: don't test Diffractor (currently broken) (\#627) (@gdalle) fix: replace use of undocumented AutoForwardDiff constructor (\#629) (@gdalle) fix: disable ForwardDiff tag checking with custom backend tags (\#631) (@gdalle) Closed issues: Error in ForwardDiff tagging (\#594) Remove undocumented ADTypes constructors (\#628) ForwardDiff Tag from prepare\_jacobian not being applied to function in jacobian! (\#630)},
  howpublished = {Zenodo},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/M2VAQQRW/14191253.html}
}

@misc{davisDrTimothyAldenDavisSuiteSparse2024,
  title = {DrTimothyAldenDavis/SuiteSparse},
  author = {Davis, Tim},
  year = {2024},
  month = may,
  url = {https://github.com/DrTimothyAldenDavis/SuiteSparse},
  urldate = {2024-05-08},
  abstract = {The official SuiteSparse library: a suite of sparse matrix algorithms authored or co-authored by Tim Davis, Texas A\&M University.},
  keywords = {tracer}
}

@misc{dixitOptimizationjlUnifiedOptimization2023,
  title = {Optimization.jl: A Unified Optimization Package},
  shorttitle = {Optimization.jl},
  author = {Dixit, Vaibhav Kumar and Rackauckas, Christopher},
  year = {2023},
  month = mar,
  doi = {10.5281/zenodo.7738525},
  url = {https://zenodo.org/records/7738525},
  urldate = {2024-11-05},
  abstract = {Optimization.jl seeks to bring together a variety of optimization packages, local and global, into one unified Julia interface. This enables a user to learn one package, and be able to use all the packages.~Optimization.jl adds a few high-level features, such as integrating with automatic differentiation,~to make its usage fairly simple for most cases, while allowing all the options in a single unified interface.},
  howpublished = {Zenodo},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/M8X4FN4B/7738525.html}
}

@article{dixonAutomaticDifferentiationLarge1990,
  title = {Automatic differentiation of large sparse systems},
  author = {Dixon, L. C. W. and Maany, Z. and Mohseninia, M.},
  year = {1990},
  month = may,
  journal = {Journal of Economic Dynamics and Control},
  series = {Special Issue on Computer Science and Economics},
  volume = {14},
  number = {2},
  pages = {299--311},
  issn = {0165-1889},
  doi = {10.1016/0165-1889(90)90023-A},
  url = {https://www.sciencedirect.com/science/article/pii/016518899090023A},
  urldate = {2024-05-08},
  abstract = {With the advent of computer languages such as ADA and PASCAL-SC that allow the definition of new data types and the overwriting of operators it has become feasible to implement new algebras on a computer relatively simply. In the field of optimization when the objective function to be minimised F(x), xϵRN, can easily involve hundreds of arithmetic operations m, the task of deriving its gradient ∇F and Hessian ∇2F can be daunting and involve many man-days of effort. The classical alternative of estimating the value of the derivatives by difference formulae can lead to numerical limitation on the performance of the code and be computationally expensive. In this paper we discuss five new algebras that make both tasks redundant as with each the computer can accurately evaluate ∇F and ∇2F in far less time than the numerical difference formulae would imply. The interface between these algebras and the Truncated Newton Algorithm for unconstrained optimisation is also described.},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/XKL8WUSB/dixon1990.pdf.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/C83LAY9K/016518899090023A.html}
}

@article{dunningJuMPModelingLanguage2017,
  title = {JuMP: A Modeling Language for Mathematical Optimization},
  shorttitle = {JuMP},
  author = {Dunning, Iain and Huchette, Joey and Lubin, Miles},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {2},
  pages = {295--320},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0036-1445},
  doi = {10/gftshn},
  url = {https://epubs.siam.org/doi/abs/10.1137/15M1020575},
  urldate = {2022-01-17},
  abstract = {JuMP is an open-source modeling language that allows users to express a wide range of optimization problems (linear, mixed-integer, quadratic, conic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic syntax. JuMP takes advantage of advanced features of the Julia programming language to offer unique functionality while achieving performance on par with commercial modeling tools for standard tasks. In this work we will provide benchmarks, present the novel aspects of the implementation, and discuss how JuMP can be extended to new problem classes and composed with state-of-the-art tools for visualization and interactivity.},
  keywords = {povar,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/QC7KARE8/Dunning et al. - 2017 - JuMP A Modeling Language for Mathematical Optimiz.pdf}
}

@mastersthesis{embayeDeterminationStructuredHessian2014,
  title = {The determination of structured Hessian matrices via automatic differentiation},
  author = {Embaye, Samuel},
  year = {2014},
  month = sep,
  url = {https://uwspace.uwaterloo.ca/handle/10012/8850},
  urldate = {2024-03-27},
  abstract = {In using automatic differentiation (AD) for Hessian computation, efficiency can be achieved by exploiting the sparsity existing in the derivative matrix. However, in the case where the Hessian is dense, this cannot be done and the space requirements to compute the Hessian can become very large. But if the underlying function can be expressed in a structured form, a “deeper” sparsity can be exploited to minimize the space requirement. In this thesis, we provide a summary of automatic differentiation (AD) techniques, as applied to Jacobian and Hessian matrix determination, as well as the graph coloring techniques involved in exploiting their sparsity. We then discuss how structure in the underlying function can be used to greatly improve efficiency in gradient/Jacobian computation. We then propose structured methods for Hessian computation that substantially reduce the space required. Finally, we propose a method for Hessian computation where the structure of the function is not provided.},
  langid = {english},
  school = {University of Waterloo},
  keywords = {tracer},
  annotation = {Accepted: 2014-09-23T17:17:37Z},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/D45JQSIF/Embaye - 2014 - The determination of structured Hessian matrices v.pdf}
}

@article{faureAutomaticDifferentiationPlatform2005,
  title = {An automatic differentiation platform: Odyssée},
  shorttitle = {An automatic differentiation platform},
  author = {Faure, Christèle},
  year = {2005},
  month = oct,
  journal = {Future Gener. Comput. Syst.},
  volume = {21},
  number = {8},
  pages = {1391--1400},
  issn = {0167-739X},
  doi = {10.1016/j.future.2004.11.006},
  abstract = {Numerous automatic differentiation strategies can be imagined to produce all kind of derivative programs under a wide range of complexity constraints, but there is no way to prototype and evaluate them on real size applications with reasonable effort. Since the development of an automatic differentiation platform is prohibitively expensive, using an existing platform to share investments between the different research teams is a good solution. Odyssee is an open automatic differentiation tool that enables the development of program analysis as well as program transformation for automatic differentiation. It has been used to differentiate large size industrial programs (300 000 lines of Fortran 77) and to prototype diverse new automatic differentiation algorithms. Its source is now freely available, a cooperative research project can therefore be based on it without financial or contractual constraint.},
  keywords = {tracer}
}

@inproceedings{fikeAutomaticDifferentiationUse2012,
  title = {Automatic Differentiation Through the Use of Hyper-Dual Numbers for Second Derivatives},
  booktitle = {Recent Advances in Algorithmic Differentiation},
  author = {Fike, Jeffrey A. and Alonso, Juan J.},
  editor = {Forth, Shaun and Hovland, Paul and Phipps, Eric and Utke, Jean and Walther, Andrea},
  year = {2012},
  pages = {163--173},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-30023-3_15},
  abstract = {Automatic Differentiation techniques are typically derived based on the chain rule of differentiation. Other methods can be derived based on the inherent mathematical properties of generalized complex numbers that enable first-derivative information to be carried in the non-real part of the number. These methods are capable of producing effectively exact derivative values. However, when second-derivative information is desired, generalized complex numbers are not sufficient. Higher-dimensional extensions of generalized complex numbers, with multiple non-real parts, can produce accurate second-derivative information provided that multiplication is commutative. One particular number system is developed, termed hyper-dual numbers, which produces exact first- and second-derivative information. The accuracy of these calculations is demonstrated on an unstructured, parallel, unsteady Reynolds-Averaged Navier-Stokes solver.},
  isbn = {978-3-642-30023-3},
  langid = {english},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/EDHTQ5ZQ/Fike and Alonso - 2012 - Automatic Differentiation Through the Use of Hyper.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/HDD9TRPL/fike2012.pdf.pdf}
}

@inproceedings{fikeDevelopmentHyperDualNumbers2011,
  title = {The Development of Hyper-Dual Numbers for Exact Second-Derivative Calculations},
  booktitle = {49th AIAA Aerospace Sciences Meeting including the New Horizons Forum and Aerospace Exposition},
  author = {Fike, Jeffrey and Alonso, Juan},
  year = {2011},
  month = jan,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  address = {Orlando, Florida},
  doi = {10.2514/6.2011-886},
  url = {https://arc.aiaa.org/doi/10.2514/6.2011-886},
  urldate = {2024-05-08},
  isbn = {978-1-60086-950-1},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/58QKXYLT/fike2011.pdf.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/MZGN2TE7/Fike and Alonso - 2011 - The Development of Hyper-Dual Numbers for Exact Se.pdf}
}

@article{forthEfficientOverloadedImplementation2006,
  title = {An efficient overloaded implementation of forward mode automatic differentiation in MATLAB},
  author = {Forth, Shaun A.},
  year = {2006},
  month = jun,
  journal = {ACM Transactions on Mathematical Software},
  volume = {32},
  number = {2},
  pages = {195--222},
  issn = {0098-3500},
  doi = {10.1145/1141885.1141888},
  url = {https://dl.acm.org/doi/10.1145/1141885.1141888},
  urldate = {2024-05-07},
  abstract = {The Mad package described here facilitates the evaluation of first derivatives of multidimensional functions that are defined by computer codes written in MATLAB. The underlying algorithm is the well-known forward mode of automatic differentiation implemented via operator overloading on variables of the class fmad. The main distinguishing feature of this MATLAB implementation is the separation of the linear combination of derivative vectors into a separate derivative vector class derivvec. This allows for the straightforward performance optimization of the overall package. Additionally, by internally using a matrix (two-dimensional) representation of arbitrary dimension directional derivatives, we may utilize MATLAB's sparse matrix class to propagate sparse directional derivatives for MATLAB code which uses arbitrary dimension arrays. On several examples, the package is shown to be more efficient than Verma's ADMAT package [Verma 1998a].},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/U5RKHTMA/Forth - 2006 - An efficient overloaded implementation of forward .pdf}
}

@article{fourerModelingLanguageMathematical1990,
  title = {A Modeling Language for Mathematical Programming},
  author = {Fourer, Robert and Gay, David M. and Kernighan, Brian W.},
  year = {1990},
  month = may,
  journal = {Management Science},
  volume = {36},
  number = {5},
  pages = {519--554},
  publisher = {INFORMS},
  issn = {0025-1909},
  doi = {10.1287/mnsc.36.5.519},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.36.5.519},
  urldate = {2024-11-05},
  abstract = {Practical large-scale mathematical programming involves more than just the application of an algorithm to minimize or maximize an objective function. Before any optimizing routine can be invoked, considerable effort must be expended to formulate the underlying model and to generate the requisite computational data structures. AMPL is a new language designed to make these steps easier and less error-prone. AMPL closely resembles the symbolic algebraic notation that many modelers use to describe mathematical programs, yet it is regular and formal enough to be processed by a computer system; it is particularly notable for the generality of its syntax and for the variety of its indexing operations. We have implemented an efficient translator that takes as input a linear AMPL model and associated data, and produces output suitable for standard linear programming optimizers. Both the language and the translator admit straightforward extensions to more general mathematical programs that incorporate nonlinear expressions or discrete variables.},
  keywords = {tracer}
}

@article{fournierADModelBuilder2012,
  title = {AD Model Builder: using automatic differentiation for statistical inference of highly parameterized complex nonlinear models},
  shorttitle = {AD Model Builder},
  author = {Fournier, David A. and Skaug, Hans J. and Ancheta, Johnoel and Ianelli, James and Magnusson, Arni and Maunder, Mark N. and Nielsen, Anders and Sibert, John},
  year = {2012},
  month = apr,
  journal = {Optimization Methods and Software},
  volume = {27},
  number = {2},
  pages = {233--249},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556788.2011.597854},
  url = {https://doi.org/10.1080/10556788.2011.597854},
  urldate = {2024-11-05},
  abstract = {Many criteria for statistical parameter estimation, such as maximum likelihood, are formulated as a nonlinear optimization problem. Automatic Differentiation Model Builder (ADMB) is a programming framework based on automatic differentiation, aimed at highly nonlinear models with a large number of parameters. The benefits of using AD are computational efficiency and high numerical accuracy, both crucial in many practical problems. We describe the basic components and the underlying philosophy of ADMB, with an emphasis on functionality found in no other statistical software. One example of such a feature is the generic implementation of Laplace approximation of high-dimensional integrals for use in latent variable models. We also review the literature in which ADMB has been used, and discuss future development of ADMB as an open source project. Overall, the main advantages of ADMB are flexibility, speed, precision, stability and built-in methods to quantify uncertainty.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/GR8LWM78/Fournier et al. - 2012 - AD Model Builder using automatic differentiation for statistical inference of highly parameterized.pdf}
}

@article{gebremedhinColPackSoftwareGraph2013,
  title = {ColPack: Software for graph coloring and related problems in scientific computing},
  shorttitle = {ColPack},
  author = {Gebremedhin, Assefaw H. and Nguyen, Duc and Patwary, Md. Mostofa Ali and Pothen, Alex},
  year = {2013},
  month = oct,
  journal = {ACM Transactions on Mathematical Software},
  volume = {40},
  number = {1},
  pages = {1:1--1:31},
  issn = {0098-3500},
  doi = {10.1145/2513109.2513110},
  url = {https://dl.acm.org/doi/10.1145/2513109.2513110},
  urldate = {2024-05-07},
  abstract = {We present a suite of fast and effective algorithms, encapsulated in a software package called ColPack, for a variety of graph coloring and related problems. Many of the coloring problems model partitioning needs arising in compression-based computation of Jacobian and Hessian matrices using Algorithmic Differentiation. Several of the coloring problems also find important applications in many areas outside derivative computation, including frequency assignment in wireless networks, scheduling, facility location, and concurrency discovery and data movement operations in parallel and distributed computing. The presentation in this article includes a high-level description of the various coloring algorithms within a common design framework, a detailed treatment of the theory and efficient implementation of known as well as new vertex ordering techniques upon which the coloring algorithms rely, a discussion of the package's software design, and an illustration of its usage. The article also includes an extensive experimental study of the major algorithms in the package using real-world as well as synthetically generated graphs.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/6KAR2CMV/Gebremedhin et al. - 2013 - ColPack Software for graph coloring and related p.pdf}
}

@article{gebremedhinEfficientComputationSparse2009,
  title = {Efficient Computation of Sparse Hessians Using Coloring and Automatic Differentiation},
  author = {Gebremedhin, Assefaw H. and Tarafdar, Arijit and Pothen, Alex and Walther, Andrea},
  year = {2009},
  month = may,
  journal = {INFORMS Journal on Computing},
  volume = {21},
  number = {2},
  pages = {209--223},
  publisher = {INFORMS},
  issn = {1091-9856},
  doi = {10.1287/ijoc.1080.0286},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/ijoc.1080.0286},
  urldate = {2024-05-07},
  abstract = {The computation of a sparse Hessian matrix H using automatic differentiation (AD) can be made efficient using the following four-step procedure: (1) Determine the sparsity structure of H, (2) obtain a seed matrix S that defines a column partition of H using a specialized coloring on the adjacency graph of H, (3) compute the compressed Hessian matrix B ≡ HS, and (4) recover the numerical values of the entries of H from B. The coloring variant used in the second step depends on whether the recovery in the fourth step is direct or indirect: a direct method uses star coloring and an indirect method uses acyclic coloring. In an earlier work, we had designed and implemented effective heuristic algorithms for these two NP-hard coloring problems. Recently, we integrated part of the developed software with the AD tool ADOL-C, which has recently acquired a sparsity detection capability. In this paper, we provide a detailed description and analysis of the recovery algorithms and experimentally demonstrate the efficacy of the coloring techniques in the overall process of computing the Hessian of a given function using ADOL-C as an example of an AD tool. We also present new analytical results on star and acyclic coloring of chordal graphs. The experimental results show that sparsity exploitation via coloring yields enormous savings in runtime and makes the computation of Hessians of very large size feasible. The results also show that evaluating a Hessian via an indirect method is often faster than a direct evaluation. This speedup is achieved without compromising numerical accuracy.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/3G6MXQI7/Gebremedhin et al. - 2009 - Efficient Computation of Sparse Hessians Using Col.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/MSDB23SG/color_version.pdf}
}

@article{gebremedhinIntroductionAlgorithmicDifferentiation2020,
  title = {An introduction to algorithmic differentiation},
  author = {Gebremedhin, Assefaw H. and Walther, Andrea},
  year = {2020},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {10},
  number = {1},
  pages = {e1334},
  issn = {1942-4795},
  doi = {10/ggbv8x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1334},
  urldate = {2021-07-17},
  abstract = {Algorithmic differentiation (AD), also known as automatic differentiation, is a technology for accurate and efficient evaluation of derivatives of a function given as a computer model. The evaluations of such models are essential building blocks in numerous scientific computing and data analysis applications, including optimization, parameter identification, sensitivity analysis, uncertainty quantification, nonlinear equation solving, and integration of differential equations. We provide an introduction to AD and present its basic ideas and techniques, some of its most important results, the implementation paradigms it relies on, the connection it has to other domains including machine learning and parallel computing, and a few of the major open problems in the area. Topics we discuss include: forward mode and reverse mode of AD, higher-order derivatives, operator overloading and source transformation, sparsity exploitation, checkpointing, cross-country mode, and differentiating iterative processes. This article is categorized under: Algorithmic Development {$>$} Scalable Statistical Methods Technologies {$>$} Data Preprocessing},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/7LAITA22/Gebremedhin_Walther_2020_An introduction to algorithmic differentiation.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/I72TXI9L/widm.html}
}

@article{gebremedhinNewAcyclicStar2007,
  title = {New Acyclic and Star Coloring Algorithms with Application to Computing Hessians},
  author = {Gebremedhin, Assefaw H. and Tarafdar, Arijit and Manne, Fredrik and Pothen, Alex},
  year = {2007},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {29},
  number = {3},
  pages = {1042--1072},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {1064-8275},
  doi = {10.1137/050639879},
  url = {https://epubs.siam.org/doi/abs/10.1137/050639879},
  urldate = {2024-05-17},
  abstract = {It is proved that every graph embedded in a fixed surface with sufficiently large edge-width is acyclically 7-colorable and that its star chromatic number is at most \$2s\_0\textasciicircum *+3\$, where \$s\_0\textasciicircum *\textbackslash leq20\$ is the maximum star chromatic number for the class of all planar graphs.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/L7GUZINC/Gebremedhin et al. - 2007 - New Acyclic and Star Coloring Algorithms with Appl.pdf}
}

@article{gebremedhinWhatColorYour2005,
  title = {What Color Is Your Jacobian? Graph Coloring for Computing Derivatives},
  shorttitle = {What Color Is Your Jacobian?},
  author = {Gebremedhin, Assefaw Hadish and Manne, Fredrik and Pothen, Alex},
  year = {2005},
  month = jan,
  journal = {SIAM Review},
  volume = {47},
  number = {4},
  pages = {629--705},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0036-1445},
  doi = {10/cmwds4},
  url = {https://epubs.siam.org/doi/abs/10.1137/S0036144504444711},
  urldate = {2022-02-03},
  abstract = {Graph coloring has been employed since the 1980s to efficiently compute sparse Jacobian and Hessian matrices using either finite differences or automatic differentiation. Several coloring problems occur in this context, depending on whether the matrix is a Jacobian or a Hessian, and on the specifics of the computational techniques employed. We consider eight variant vertex coloring problems here. This article begins with a gentle introduction to the problem of computing a sparse Jacobian, followed by an overview of the historical development of the research area. Then we present a unifying framework for the graph models of the variant matrix estimation problems. The framework is based upon the viewpoint that a partition of a matrix into structurally orthogonal groups of columns corresponds to distance-2 coloring an appropriate graph representation. The unified framework helps integrate earlier work and leads to fresh insights; enables the design of more efficient algorithms for many problems; leads to new algorithms for others; and eases the task of building graph models for new problems. We report computational results on two of the coloring problems to support our claims. Most of the methods for these problems treat a column or a row of a matrix as an atomic entity, and partition the columns or rows (or both). A brief review of methods that do not fit these criteria is provided. We also discuss results in discrete mathematics and theoretical computer science that intersect with the topics considered here.},
  keywords = {autodiff,semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/AGLB9RM8/Gebremedhin et al_2005_What Color Is Your Jacobian.pdf}
}

@inproceedings{geitnerAutomaticComputationSparse1995,
  title = {Automatic Computation of Sparse Jacobians by Applying the Method of Newsam and Ramsdell},
  author = {Geitner, U. and Utke, J. and Griewank, A.},
  year = {1995},
  url = {https://www.semanticscholar.org/paper/Automatic-Computation-of-Sparse-Jacobians-by-the-of-Geitner-Utke/1ed218348fff39e9642d7b7ac38cf0dd66aea47b},
  urldate = {2024-05-08},
  abstract = {The computation of sparse Jacobians is a common subproblem in iterative numerical algorithms. The sparsity structure is not always known a priori and may sometimes change from point to point. The subject of this paper is the automatic detection of the sparsity structure and its exploitation for an eecient computation of Jacobians using automatic diierentiation, graph coloring, and fast solution algorithms for Vandermonde systems. Many numerical algorithms for the solution of optimization problems, nonlinear algebraic systems, or diierential equations require the computation of Jacobians or other derivatives. In virtually all cases the function of interest f : I R n 7 ! I R m is given as an evaluation routine in a computer programming language. Apart from hand-coded derivatives, symbolic diierentiation, and divided diierences, automatic diierentiation (AD) is an eecient way to obtain derivative values. AD uses the execution trace of the evaluation routine to compute derivative values at a given argument point. The computational eeort, which depends on the number of dependent and independent variables, is clearly predictable and often less than the eeort for divided diierences. In addition, automatic diierentiation is exact up to the precision of the underlying subroutine; in other words, the truncation error incurred by divided diierences is avoided. The sparsity structure of the Jacobian J = f 0 (x) of y = f(x) is determined by the dependency relation between the independent variables x and the dependent variables y. The computational eeort of symbolic diierentiation is dominated by the (typically exponential) expression swell, while hand-coded derivatives are error prone, and their generation is time consuming. Automatic diierentiation tools are based on the code of the evaluation routine. Therefore, information about the dependency relation is available. It is a natural idea to ask for this qualitative dependency information before proceeding to the actual evaluation. (We do not intend to explain the theory of automatic diierentiation and its application. One may refer to M. Iri's contribution in 1], L. Rall's introduction in this volume, or 13] for more detailed introductions.) Four facts are essential in our context: (f1) there are two modes of automatic diierentiation, the forward and the reverse modes, both of which have predictable temporal and spatial complexity; (f2) the forward mode can compute linear combinations of columns (Js) of the Jacobian 1 with a complexity that does not depend on m; 1 We can look at these linear combinations as directional derivatives …},
  keywords = {⛔ No DOI found,todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/46A6X7IF/Geitner et al. - Automatic Computation of Sparse Jacobians by Apply.pdf}
}

@inproceedings{gieringAutomaticSparsityDetection2006,
  title = {Automatic Sparsity Detection Implemented as a Source-to-Source Transformation},
  booktitle = {Computational Science – ICCS 2006},
  author = {Giering, Ralf and Kaminski, Thomas},
  editor = {Alexandrov, Vassil N. and {van Albada}, Geert Dick and Sloot, Peter M. A. and Dongarra, Jack},
  year = {2006},
  pages = {591--598},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11758549_81},
  abstract = {An implementation of Automatic Sparsity Detection (ASD) as a new source-to-source transformation is presented. Given a code for evaluation of a function, ASD generates code to evaluate the sparsity pattern of the function’s Jacobian by operations on bit-vectors. Similar to Automatic Differentiation (AD), there are forward and reverse modes of ASD. As ASD code has significantly fewer required variables than AD, ASD should be operated in pure mode, i.e. without an evaluation of the underlying function included in the ASD code. In a performance comparison of ASD to AD on five small test problems, ASD is about two orders of magnitude faster than AD. Hence, for a particular class of sparse Jacobians, it is efficient to determine first the sparsity patten via ASD. In a subsequent AD step, this allows to reduce the effective dimension for the evaluation of the Jacobian by avoiding the evaluation of zero elements via a selection of seed matrices according to the sparsity pattern.},
  isbn = {978-3-540-34386-8},
  langid = {english},
  keywords = {done,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/T6YNUZMV/Giering and Kaminski - 2006 - Automatic Sparsity Detection Implemented as a Sour.pdf}
}

@article{gieringGeneratingEfficientDerivative2005,
  title = {Generating efficient derivative code with TAF: Adjoint and tangent linear Euler flow around an airfoil},
  shorttitle = {Generating efficient derivative code with TAF},
  author = {Giering, R. and Kaminski, T. and Slawig, T.},
  year = {2005},
  month = oct,
  journal = {Future Generation Computer Systems},
  volume = {21},
  number = {8},
  pages = {1345--1355},
  issn = {0167-739X},
  doi = {10.1016/j.future.2004.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X04001785},
  urldate = {2024-05-08},
  abstract = {FastOpt’s new automatic differentiation tool TAF is applied to the two-dimensional Navier–Stokes solver NSC2KE. For a configuration that simulates the Euler flow around an NACA airfoil, TAF has generated the tangent linear and adjoint models as well as the second derivative (Hessian) code. Owing to TAF’s capability of generating efficient adjoints of iterative solvers, the derivative code has a high performance: running both the solver and its adjoint requires 3.4 times as long as running the solver only. Further examples of highly efficient tangent linear, adjoint, and Hessian codes for large and complex three-dimensional Fortran 77-90 climate models are listed. These examples suggest that the performance of the NSC2KE adjoint may well be generalised to more complex three-dimensional CFD codes. We also sketch how TAF can improve the adjoint’s performance by exploiting self-adjointness, which is a common feature of CFD codes.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/8BMDAQS8/giering2005.pdf.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/CFFU3SHV/Giering et al. - 2005 - Generating efficient derivative code with TAF Adj.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/3MHCVA8M/S0167739X04001785.html}
}

@article{gilesExtendedCollectionMatrix2008,
  title = {An extended collection of matrix derivative results for forward and reverse mode automatic differentiation},
  author = {Giles, M.},
  year = {2008},
  publisher = {Unspecified},
  url = {https://ora.ox.ac.uk/objects/uuid:8d0c0a29-c92b-4153-a1d2-38b276e93124},
  urldate = {2024-07-29},
  abstract = {This paper collects together a number of matrix derivative results which are very useful in forward and reverse mode algorithmic differentiation (AD). It highlights in particular the remarkable contribution of a 1948 paper by Dwyer and Macphail which derives the linear and adjoint sensitivities of a matrix product, inverse and determinant, and a number of related results motivated by applications in multivariate analysis in statistics. This is an extended version of a paper which will appear in the proceedings of AD2008, the 5th International Conference on Automatic Differentiation.},
  langid = {english},
  keywords = {⛔ No DOI found,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/W3BB2Z53/Giles_2008_An extended collection of matrix derivative results for forward and reverse.pdf}
}

@inproceedings{gillisHierarchicalSeedingEfficient2014,
  title = {Hierarchical seeding for efficient sparsity pattern recovery in automatic differentiation},
  booktitle = {Book of Abstracts of the Sixth SIAM Workshop of Combinatorial Scientific Computing (2014)},
  author = {Gillis, Joris and Diehl, Moritz},
  year = {2014},
  volume = {82},
  pages = {14--15},
  keywords = {⛔ No DOI found,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/Y9CDVMIL/Gillis and Diehl - Hierarchical seeding for eﬃcient sparsity pattern .pdf}
}

@article{girolamiRiemannManifoldLangevin2011,
  title = {Riemann Manifold Langevin and Hamiltonian Monte Carlo Methods},
  author = {Girolami, Mark and Calderhead, Ben},
  year = {2011},
  month = mar,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {73},
  number = {2},
  pages = {123--214},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2010.00765.x},
  url = {https://doi.org/10.1111/j.1467-9868.2010.00765.x},
  urldate = {2024-05-16},
  abstract = {The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis–Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from http://www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/AIM4PXIX/Girolami_Calderhead_2011_Riemann Manifold Langevin and Hamiltonian Monte Carlo Methods.pdf}
}

@article{gowdaHighperformanceSymbolicnumericsMultiple2022,
  title = {High-performance symbolic-numerics via multiple dispatch},
  author = {Gowda, Shashi and Ma, Yingbo and Cheli, Alessandro and Gwóźzdź, Maja and Shah, Viral B. and Edelman, Alan and Rackauckas, Christopher},
  year = {2022},
  month = jan,
  journal = {ACM Commun. Comput. Algebra},
  volume = {55},
  number = {3},
  pages = {92--96},
  issn = {1932-2232},
  doi = {10.1145/3511528.3511535},
  url = {https://dl.acm.org/doi/10.1145/3511528.3511535},
  urldate = {2024-11-05},
  abstract = {As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. Exploiting this feature, we demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We illustrate how this symbolic system improves numerical computing tasks by showcasing an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/65CD277B/Gowda et al. - 2022 - High-performance symbolic-numerics via multiple dispatch.pdf}
}

@inproceedings{gowdaSparsityProgrammingAutomated2019,
  title = {Sparsity Programming: Automated Sparsity-Aware Optimizations in Differentiable Programming},
  shorttitle = {Sparsity Programming},
  booktitle = {Program Transformations for ML Workshop at NeurIPS 2019},
  author = {Gowda, Shashi and Ma, Yingbo and Churavy, Valentin and Edelman, Alan and Rackauckas, Christopher},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=rJlPdcY38B},
  urldate = {2024-03-27},
  abstract = {Previous studies in numerical analysis have shown how the calculation of a Jacobian, Hessian, and their factorizations can be accelerated when their sparsity pattern is known. However, accurate Jacobian and Hessian sparsity patterns cannot be computed numerically, leaving the burden on the user to provide them. As scientific simulations have grown in complexity, the application of differentiable programming for calculating the derivatives of arbitrary programs with respect to parameters has been on the rise, but current methodologies do not automatically detect and make use of sparsity-related accelerations. In this manuscript we develop a method for the accurate and efficient construction of sparsity patterns by transforming an input program into one that computes the sparsity pattern of its Jacobian or Hessian. Our implementation, which we demonstrate on partial differential equations, is a scalable technique for acceleration of automatic differentiation on arbitrarily complex multivariate programs. This demonstrates that dynamic program analysis can be effective in more scenarios than are currently well known in differentiable programming.},
  langid = {english},
  keywords = {done,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/4N3SHJ4L/Gowda et al. - 2019 - Sparsity Programming Automated Sparsity-Aware Opt.pdf}
}

@article{gowerComputingSparsityPattern2014,
  title = {Computing the sparsity pattern of Hessians using automatic differentiation},
  author = {Gower, Robert Mansel and Mello, Margarida Pinheiro},
  year = {2014},
  month = mar,
  journal = {ACM Transactions on Mathematical Software},
  volume = {40},
  number = {2},
  pages = {10:1--10:15},
  issn = {0098-3500},
  doi = {10.1145/2490254},
  url = {https://dl.acm.org/doi/10.1145/2490254},
  urldate = {2024-03-27},
  abstract = {We compare two methods that calculate the sparsity pattern of Hessian matrices using the computational framework of automatic differentiation. The first method is a forward-mode algorithm by Andrea Walther in 2008 which has been implemented as the driver called hess\_pat in the automatic differentiation package ADOL-C. The second is edge\_push\_sp, a new reverse mode algorithm descended from the edge\_pushing algorithm for calculating Hessians by Gower and Mello in 2012. We present complexity analysis and perform numerical tests for both algorithms. The results show that the new reverse algorithm is very promising.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/P4QHBTVR/Gower and Mello - 2014 - Computing the sparsity pattern of Hessians using a.pdf}
}

@article{gowerNewFrameworkComputation2012,
  title = {A new framework for the computation of Hessians},
  author = {Gower, R. M. and Mello, M. P.},
  year = {2012},
  month = apr,
  journal = {Optimization Methods and Software},
  volume = {27},
  number = {2},
  pages = {251--273},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556788.2011.580098},
  url = {https://doi.org/10.1080/10556788.2011.580098},
  urldate = {2024-05-07},
  abstract = {We investigate the computation of Hessian matrices via Automatic Differentiation, using a graph model and an algebraic model. The graph model reveals the inherent symmetries involved in calculating the Hessian. The algebraic model, based on Griewank and Walther's [Evaluating derivatives, in Principles and Techniques of Algorithmic Differentiation, 2nd ed., Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2008] state transformations synthesizes the calculation of the Hessian as a formula. These dual points of view, graphical and algebraic, lead to a new framework for Hessian computation. This is illustrated by developing edge\_pushing, a new truly reverse Hessian computation algorithm that fully exploits the Hessian's symmetry. Computational experiments compare the performance of edge\_pushing on 16 functions from the CUTE collection [I. Bongartz et al. Cute: constrained and unconstrained testing environment, ACM Trans. Math. Softw. 21(1) (1995), pp. 123–160] against two algorithms available as drivers of the software ADOL-C [A. Griewank et al. ADOL-C: A package for the automatic differentiation of algorithms written in C/C++, Technical report, Institute of Scientific Computing, Technical University Dresden, 1999. Updated version of the paper published in ACM Trans. Math. Softw. 22, 1996, pp. 131–167; A. Walther, Computing sparse Hessians with automatic differentiation, ACM Trans. Math. Softw. 34(1) (2008), pp. 1–15; A.H. Gebremedhin et al. Efficient computation of sparse Hessians using coloring and automatic differentiation, INFORMS J. Comput. 21(2) (2009), pp. 209–223], and the results are very promising.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/HWT7JHGW/Gower and Mello - 2012 - A new framework for the computation of Hessians.pdf}
}

@article{griewankAlgorithm755ADOLC1996,
  title = {Algorithm 755: ADOL-C: a package for the automatic differentiation of algorithms written in C/C++},
  shorttitle = {Algorithm 755},
  author = {Griewank, Andreas and Juedes, David and Utke, Jean},
  year = {1996},
  month = jun,
  journal = {ACM Transactions on Mathematical Software},
  volume = {22},
  number = {2},
  pages = {131--167},
  issn = {0098-3500},
  doi = {10.1145/229473.229474},
  url = {https://dl.acm.org/doi/10.1145/229473.229474},
  urldate = {2024-05-08},
  abstract = {The C++ package ADOL-C described here facilitates the evaluation of first and higher derivatives of vector functions that are defined by computer programs written in C or C++. The resulting derivative evaluation routines may be called from C/C++, Fortran, or any other language that can be linked with C. The numerical values of derivative vectors are obtained free of truncation errors at a small multiple of the run-time and randomly accessed memory of the given function evaluation program. Derivative matrices are obtained by columns or rows. For solution curves defined by ordinary differential equations, special routines are provided that evaluate the Taylor coefficient vectors and their Jacobians with respect to the current state vector. The derivative calculations involve a possibly substantial (but always predictable) amount of data that are accessed strictly sequentially and are therefore automatically paged out to external files.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/4Q6JN39C/griewank1996.pdf.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/EICN4KD2/Griewank et al. - 1996 - Algorithm 755 ADOL-C a package for the automatic.pdf}
}

@techreport{griewankCalculationJacobianMatrices1991,
  title = {On the calculation of Jacobian matrices by the Markowitz rule},
  author = {Griewank, A. and Reese, S.},
  year = {1991},
  month = dec,
  number = {ANL/CP-75176; CONF-910189-4},
  institution = {Argonne National Lab., IL (United States)},
  url = {https://www.osti.gov/biblio/10118065},
  urldate = {2024-05-08},
  abstract = {The evaluation of derivative vectors can be performed with optimal computational complexity by the forward or reverse mode of automatic differentiation. This approach may be applied to evaluate first and higher derivatives of any vector function that is defined as the composition of easily differentiated elementary functions, typically in the form of a computer program. The more general task of efficiently evaluating Jacobians or other derivative matrices leads to a combinational optimization problem, which is conjectured to be NP-hard. Here, we examine this vertex elimination problem and solve it approximately, using a greedy heuristic. Numerical experiments show the resulting Markowitz scheme for Jacobian evaluation to be more efficient than column by column or row by row evaluation using the forward or the reverse mode, respectively.},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/AG4KLAK7/Griewank and Reese - 1991 - On the calculation of Jacobian matrices by the Mar.pdf}
}

@article{griewankDetectingJacobianSparsity2002,
  title = {Detecting Jacobian sparsity patterns by Bayesian probing},
  author = {Griewank, Andreas and Mitev, Christo},
  year = {2002},
  month = jun,
  journal = {Mathematical Programming},
  volume = {93},
  number = {1},
  pages = {1--25},
  issn = {1436-4646},
  doi = {10.1007/s101070100281},
  url = {https://doi.org/10.1007/s101070100281},
  urldate = {2024-09-06},
  abstract = {In this paper we describe an automatic procedure for successively reducing the set of possible nonzeros in a Jacobian matrix until eventually the exact sparsity pattern is obtained. The dependence information needed in this probing process consist of “Boolean” Jacobian-vector products and possibly also vector-Jacobian products, which can be evaluated exactly by automatic differentiation or approximated by divided differences. The latter approach yields correct sparsity patterns, provided there is no exact cancellation at the current argument.¶Starting from a user specified, or by default initialized, probability distribution the procedure suggests a sequence of probing vectors. The resulting information is then used to update the probabilities that certain elements are nonzero according to Bayes’ law. The proposed probing procedure is found to require only O(logn) probing vectors on randomly generated matrices of dimension n, with a fixed number of nonzeros per row or column. This result has been proven for (block-) banded matrices, and for general sparsity pattern finite termination of the probing procedure can be guaranteed.},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/84W4FPNL/Griewank_Mitev_2002_Detecting Jacobian sparsity patterns by Bayesian probing.pdf}
}

@book{griewankEvaluatingDerivativesPrinciples2008,
  title = {Evaluating derivatives: principles and techniques of algorithmic differentiation},
  shorttitle = {Evaluating derivatives},
  author = {Griewank, Andreas and Walther, Andrea},
  year = {2008},
  edition = {2nd ed},
  publisher = {Society for Industrial and Applied Mathematics},
  address = {Philadelphia, PA},
  abstract = {This title is a comprehensive treatment of algorithmic, or automatic, differentiation. The second edition covers recent developments in applications and theory, including an elegant NP completeness argument and an introduction to scarcity},
  isbn = {978-0-89871-659-7},
  lccn = {QA304 .G76 2008},
  keywords = {autodiff,inferopt,semidone,thesis,tracer},
  annotation = {OCLC: ocn227574816},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/ZBNZIFGC/Griewank_Walther_2008_Evaluating Derivatives.pdf}
}

@article{griewankEvaluatingHigherDerivative2000,
  title = {Evaluating higher derivative tensors by forward propagation of univariate Taylor series},
  author = {Griewank, Andreas and Utke, Jean and Walther, Andrea},
  year = {2000},
  month = jul,
  journal = {Mathematics of Computation},
  volume = {69},
  number = {231},
  pages = {1117--1130},
  issn = {0025-5718},
  doi = {10.1090/S0025-5718-00-01120-0},
  url = {https://doi.org/10.1090/S0025-5718-00-01120-0},
  urldate = {2024-06-05},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/2KRHV3F7/Griewank et al. - 2000 - Evaluating higher derivative tensors by forward pr.pdf}
}

@article{griewankStablePiecewiseLinearization2013,
  title = {On stable piecewise linearization and generalized algorithmic differentiation},
  author = {Griewank, Andreas},
  year = {2013},
  month = dec,
  journal = {Optimization Methods and Software},
  volume = {28},
  number = {6},
  pages = {1139--1178},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556788.2013.796683},
  url = {https://doi.org/10.1080/10556788.2013.796683},
  urldate = {2024-06-05},
  abstract = {It is shown how functions that are defined by evaluation programs involving the absolute value function abs() (besides smooth elementals) can be approximated locally by piecewise-linear models in the style of algorithmic or automatic differentiation (AD). The model can be generated by a minor modification of standard AD tools and it is Lipschitz continuous with respect to the base point at which it is developed. The discrepancy between the original function, which is piecewise differentiable, and the piecewise linear model is of second order in the distance to the base point. Consequently, successive piecewise linearization yields bundle type methods for unconstrained minimization and Newton-type equation solvers. As a third fundamental numerical task we consider the integration of ordinary differential equations, for which we examine generalizations of the midpoint and the trapezoidal rule for the case of Lipschitz continuous right hand sides (RHSs). As a by-product of piecewise linearization, we show how to compute at any base point some generalized Jacobians of the original function, namely those that are conically active as defined by Khan and Barton. This subset of the Clarke Jacobian is never empty, independent of the particular function representation in terms of elementals, and also invariant with respect to linear transformations on domain and range. However, like all generalized derivatives the conically active Jacobians reduce almost everywhere to the singleton formed by the proper Jacobian, which may approximate the original function only in a minuscule neighbourhood. Since the piecewise linearization always reflects kinks in the vicinity, we illustrate how it can be used to approximate generalized Jacobians at nearby points along a user specified preferred direction.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/7LJ9TBTI/Griewank - 2013 - On stable piecewise linearization and generalized .pdf}
}

@incollection{griewankUnconstrainedOptimizationPartially1982,
  title = {On the unconstrained optimization of partially separable functions},
  booktitle = {Nonlinear Optimization 1981},
  author = {Griewank, Andreas and Toint, Philippe},
  editor = {Powell, M. J. D},
  year = {1982},
  pages = {301--312},
  publisher = {Academic press},
  address = {London},
  keywords = {tracer}
}

@incollection{griewankWhoInventedReverse2012,
  title = {Who invented the reverse mode of differentiation?},
  booktitle = {Optimization Stories},
  author = {Griewank, Andreas},
  editor = {Grötschel, Martin},
  year = {2012},
  month = jan,
  edition = {1},
  pages = {389--400},
  publisher = {EMS Press},
  doi = {10.4171/dms/6/38},
  url = {https://ems.press/doi/10.4171/dms/6/38},
  urldate = {2024-03-07},
  isbn = {978-3-936609-58-5 978-3-9854754-0-7},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/PT2GJV6Z/Griewank - 2012 - Who invented the reverse mode of differentiation.pdf}
}

@article{guoSchemeAutomaticDifferentiation2021,
  title = {A scheme for automatic differentiation of complex loss functions},
  author = {Guo, Chu and Poletti, Dario},
  year = {2021},
  month = jan,
  journal = {Physical Review E},
  volume = {103},
  number = {1},
  eprint = {2003.04295},
  primaryclass = {cs},
  pages = {013309},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.103.013309},
  url = {http://arxiv.org/abs/2003.04295},
  urldate = {2024-05-23},
  abstract = {For a real function, automatic differentiation is such a standard algorithm used to efficiently compute its gradient, that it is integrated in various neural network frameworks. However, despite the recent advances in using complex functions in machine learning and the well-established usefulness of automatic differentiation, the support of automatic differentiation for complex functions is not as well-established and widespread as for real functions. In this work we propose an efficient and seamless scheme to implement automatic differentiation for complex functions, which is a compatible generalization of the current scheme for real functions. This scheme can significantly simplify the implementation of neural networks which use complex numbers.},
  archiveprefix = {arXiv},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/48XBJDNL/Guo and Poletti - 2021 - A scheme for automatic differentiation of complex .pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/S9QSRB4N/2003.html}
}

@inproceedings{hasanDSJMSoftwareToolkit2016,
  title = {DSJM: A Software Toolkit for Direct Determination of Sparse Jacobian Matrices},
  shorttitle = {DSJM},
  booktitle = {Mathematical Software – ICMS 2016},
  author = {Hasan, Mahmudul and Hossain, Shahadat and Khan, Ahamad Imtiaz and Mithila, Nasrin Hakim and Suny, Ashraful Huq},
  editor = {Greuel, Gert-Martin and Koch, Thorsten and Paule, Peter and Sommese, Andrew},
  year = {2016},
  pages = {275--283},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-42432-3_34},
  abstract = {We describe the main design features of DSJM (Determine Sparse Jacobian Matrices), a software toolkit written in standard C++ that enables direct determination of sparse Jacobian matrices. Our design exploits the recently proposed unifying framework “pattern graph” and employs cache-friendly array-based sparse data structures. The DSJM implements a greedy grouping (coloring) algorithm and several ordering heuristics. In our numerical testing on a suite of large-scale test instances DSJM consistently produced better timing and partitions compared with a similar software.},
  isbn = {978-3-319-42432-3},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/KM6E7Y7X/Hasan et al. - 2016 - DSJM A Software Toolkit for Direct Determination .pdf}
}

@article{herholzSparsitySpecificCodeOptimization2022,
  title = {Sparsity-Specific Code Optimization using Expression Trees},
  author = {Herholz, Philipp and Tang, Xuan and Schneider, Teseo and Kamil, Shoaib and Panozzo, Daniele and {Sorkine-Hornung}, Olga},
  year = {2022},
  month = may,
  journal = {ACM Trans. Graph.},
  volume = {41},
  number = {5},
  pages = {175:1--175:19},
  issn = {0730-0301},
  doi = {10.1145/3520484},
  url = {https://dl.acm.org/doi/10.1145/3520484},
  urldate = {2024-11-05},
  abstract = {We introduce a code generator that converts unoptimized C++ code operating on sparse data into vectorized and parallel CPU or GPU kernels. Our approach unrolls the computation into a massive expression graph, performs redundant expression elimination, grouping, and then generates an architecture-specific kernel to solve the same problem, assuming that the sparsity pattern is fixed, which is a common scenario in many applications in computer graphics and scientific computing. We show that our approach scales to large problems and can achieve speedups of two orders of magnitude on CPUs and three orders of magnitude on GPUs, compared to a set of manually optimized CPU baselines. To demonstrate the practical applicability of our approach, we employ it to optimize popular algorithms with applications to physical simulation and interactive mesh deformation.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/SFTWQDE6/Herholz et al. - 2022 - Sparsity-Specific Code Optimization using Expression Trees.pdf}
}

@article{hestenesMethodsConjugateGradients1952,
  title = {Methods of conjugate gradients for solving linear systems},
  author = {Hestenes, M.R. and Stiefel, E.},
  year = {1952},
  month = dec,
  journal = {Journal of Research of the National Bureau of Standards},
  volume = {49},
  number = {6},
  pages = {409},
  issn = {0091-0635},
  doi = {10.6028/jres.049.044},
  url = {https://nvlpubs.nist.gov/nistpubs/jres/049/jresv49n6p409_A1b.pdf},
  urldate = {2024-05-07},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/5CRNDIGZ/Hestenes and Stiefel - 1952 - Methods of conjugate gradients for solving linear .pdf}
}

@article{hillAutomaticallyFindingExploiting1996,
  title = {Automatically Finding and Exploiting Partially Separable Structure in Nonlinear Programming Problems},
  author = {Hill, Murray},
  year = {1996},
  url = {https://www.ampl.com/wp-content/uploads/psstruc.pdf},
  abstract = {Nonlinear programming problems often involve an objective and constraints that are partially separable — the sum of terms involving only a few variables (perhaps after a linear change of variables). This paper discusses finding and exploiting such structure in nonlinear programming problems expressed symbolically in the AMPL modeling language. For some computations, such as computing Hessians by backwards automatic differentiation, exploiting partial separability can give significant speedups.},
  langid = {english},
  keywords = {⛔ No DOI found,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/UV5NMKSI/Laboratories and Hill - 1996 - Automatically Finding and Exploiting Partially Sep.pdf}
}

@inproceedings{hillMoreADNonlinear1996,
  title = {More AD of Nonlinear AMPL Models: Computing Hessian Information and Exploiting Partial Separability},
  booktitle = {Second International Workshop on Com- putational Differentiation},
  author = {Hill, Murray},
  year = {1996},
  address = {Santa Fe, New Mexico},
  url = {https://www.ampl.com/wp-content/uploads/1996-Gay-More-AD-of-Nonlinear-AMPL-Models.pdf},
  abstract = {We describe computational experience with automatic differentiation of mathematical programming problems expressed in the modeling language AMPL. Nonlinear expressions are translated to loop-free code, which makes it easy to compute gradients and Jacobians by backward automatic differentiation. The nonlinear expressions may be interpreted or, to gain some evaluation speed at the cost of increased preparation time, converted to Fortran or C. We have extended the interpretive scheme to evaluate Hessian (of Lagrangian) times vector. Detecting partially separable structure (sums of terms, each depending, perhaps after a linear transformation, on only a few variables) is of independent interest, as some solvers exploit this structure. It can be detected automatically by suitable ‘‘tree walks’’. Exploiting this structure permits an AD computation of the entire Hessian matrix by accumulating Hessian times vector computations for each term, and can lead to a much faster computation of the Hessian than by computing the whole Hessian times each unit vector.},
  langid = {english},
  keywords = {⛔ No DOI found,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/IMUUSULJ/Laboratories and Hill - 1996 - More AD of Nonlinear AMPL Models Computing Hessia.pdf}
}

@misc{hillSparseConnectivityTracerjl2024,
  title = {SparseConnectivityTracer.jl},
  author = {Hill, Adrian and Dalle, Guillaume},
  year = {2024},
  month = oct,
  doi = {10.5281/zenodo.13961066},
  url = {https://zenodo.org/records/13961066},
  urldate = {2024-11-04},
  abstract = {SparseConnectivityTracer v0.6.8 Diff since v0.6.7 Merged pull requests: Increase code coverage (\#206) (@adrhill) Support clamp and clamp! (\#208) (@adrhill) Remove DuplicateVector (\#209) (@adrhill) Closed issues: Fix DuplicateVector performance (\#63) Overload lu on matrices of tracers (\#138) Specialize array overloads (\#192) clamp gives TypeError from julia 1.11 (\#207)},
  howpublished = {Zenodo},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/BFFBYZ3M/13961066.html}
}

@article{hossainComputingSparseJacobian1998,
  title = {Computing a sparse Jacobian matrix by rows and columns},
  author = {Hossain, A. K. M. Shahadat and Steihaug, Trond},
  year = {1998},
  month = jan,
  journal = {Optimization Methods and Software},
  volume = {10},
  number = {1},
  pages = {33--48},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556789808805700},
  url = {https://doi.org/10.1080/10556789808805700},
  urldate = {2024-11-09},
  abstract = {Efficient estimation of large sparse Jacobian matrices has been studied extensively in the last couple of years. It has been observed that the estimation of Jacobian matrix can be posed as a graph coloring problem. Elements of the matrix are estimated by taking divided difference in several directions corresponding to a group of structurally independent columns. Another possibility is to obtain the nonzero elements by means of the so called Automatic differentiation, which gives the estimates free of truncation error that one encounters in a divided difference scheme. In this paper we show that it is possible to exploit sparsity both in columns and rows by employing the forward and the reverse mode of Automatic differentiation. A graph-theoretic characterization of the problem is given.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/YP2BT6HT/Hossain and Steihaug - 1998 - Computing a sparse Jacobian matrix by rows and columns.pdf}
}

@misc{huckelheimUnderstandingAutomaticDifferentiation2023,
  title = {Understanding Automatic Differentiation Pitfalls},
  author = {Hückelheim, Jan and Menon, Harshitha and Moses, William and Christianson, Bruce and Hovland, Paul and Hascoët, Laurent},
  year = {2023},
  month = may,
  number = {arXiv:2305.07546},
  eprint = {2305.07546},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07546},
  url = {http://arxiv.org/abs/2305.07546},
  urldate = {2024-11-05},
  abstract = {Automatic differentiation, also known as backpropagation, AD, autodiff, or algorithmic differentiation, is a popular technique for computing derivatives of computer programs accurately and efficiently. Sometimes, however, the derivatives computed by AD could be interpreted as incorrect. These pitfalls occur systematically across tools and approaches. In this paper we broadly categorize problematic usages of AD and illustrate each category with examples such as chaos, time-averaged oscillations, discretizations, fixed-point loops, lookup tables, and linear solvers. We also review debugging techniques and their effectiveness in these situations. With this article we hope to help readers avoid unexpected behavior, detect problems more easily when they occur, and have more realistic expectations from AD tools.},
  archiveprefix = {arXiv},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/K4DFBVM7/Hückelheim et al. - 2023 - Understanding Automatic Differentiation Pitfalls.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/BQBZAZVG/2305.html}
}

@misc{innesDifferentiableProgrammingSystem2019a,
  title = {A Differentiable Programming System to Bridge Machine Learning and Scientific Computing},
  author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Chris and Saba, Elliot and Shah, Viral B. and Tebbutt, Will},
  year = {2019},
  month = jul,
  number = {arXiv:1907.07587},
  eprint = {1907.07587},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.07587},
  url = {http://arxiv.org/abs/1907.07587},
  urldate = {2024-10-27},
  abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe Zygote, a Differentiable Programming system that is able to take gradients of general program structures. We implement this system in the Julia programming language. Our system supports almost all language constructs (control flow, recursion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning, but more importantly, it enables us to incorporate a large ecosystem of libraries in our models in a straightforward way. We discuss our approach to automatic differentiation, including its support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present several examples of differentiating programs.},
  archiveprefix = {arXiv},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/QCLS47SD/Innes et al. - 2019 - A Differentiable Programming System to Bridge Machine Learning and Scientific Computing.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/MN5BVRKM/1907.html}
}

@misc{innesDontUnrollAdjoint2019,
  title = {Don't Unroll Adjoint: Differentiating SSA-Form Programs},
  shorttitle = {Don't Unroll Adjoint},
  author = {Innes, Michael},
  year = {2019},
  month = mar,
  number = {arXiv:1810.07951},
  eprint = {1810.07951},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1810.07951},
  urldate = {2022-10-13},
  abstract = {This paper presents reverse-mode algorithmic differentiation (AD) based on source code transformation, in particular of the Static Single Assignment (SSA) form used by modern compilers. The approach can support control flow, nesting, mutation, recursion, data structures, higher-order functions, and other language constructs, and the output is given to an existing compiler to produce highly efficient differentiated code. Our implementation is a new AD tool for the Julia language, called Zygote, which presents high-level dynamic semantics while transparently compiling adjoint code under the hood. We discuss the benefits of this approach to both the usability and performance of AD tools.},
  archiveprefix = {arXiv},
  keywords = {inferopt,thesis,todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/YL8KJNJF/Innes_2019_Don't Unroll Adjoint.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/G4B4MM9Z/1810.html}
}

@phdthesis{jesminImprovedImplementationSparsity2018,
  title = {An improved implementation of sparsity detection of sparse derivative matrices},
  author = {Jesmin, Tasnuba},
  year = {2018},
  url = {https://hdl.handle.net/10133/5266},
  urldate = {2024-05-07},
  abstract = {Optimization is a crucial branch of research with application in numerous domain. Determination of sparsity is a vital stream of optimization research with potentials for improvement. Manual determination of sparsity structure of Jacobian matrix for a large problem is complicated and highly error-prone. The main motivation of this research is to propose an efficient algorithm which can effectively detect and represent sparsity of unknown Jacobian matrices. Automated sparsity detection algorithms find an optimal or near-optimal solution, which reduces time and space complexity for large scale data. Our proposed approach efficiently generates symmetric pattern utilizing band matrix and reduces the number of gradient evaluation. For efficient solution, we integrate our approach with existing pattern detection process. Greedy coloring algorithm is used for column portioning and multilevel algorithm with voting scheme is implemented for detection of sparsity pattern. Finally, parallel computation is used to reduce processing time of the overall approach.},
  langid = {american},
  school = {Lethbridge, Alta. : Universtiy of Lethbridge, Department of Mathematics and Computer Science},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/TCG49U6M/Jesmin and Science - 2018 - An improved implementation of sparsity detection o.pdf}
}

@article{juedesColoringJacobiansRevisited2012,
  title = {Coloring Jacobians revisited: a new algorithm for star and acyclic bicoloring},
  shorttitle = {Coloring Jacobians revisited},
  author = {Juedes, David and Jones, Jeffrey},
  year = {2012},
  month = apr,
  journal = {Optimization Methods and Software},
  volume = {27},
  number = {2},
  pages = {295--309},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556788.2011.606575},
  url = {https://doi.org/10.1080/10556788.2011.606575},
  urldate = {2024-07-18},
  abstract = {This paper presents a new polynomial-time algorithm, approximate star bicoloring (ASBC), for star bicoloring and acyclic bicoloring. These NP-complete combinatorial problems arise from problems associated with computing large sparse Jacobian matrices. The main results of this paper lie in approximation analysis related to these problems. In particular, it is shown that (i) both star and acyclic bicoloring can be approximated in polynomial time to the ratio O(n 2/3) and (ii) both star and acyclic bicoloring cannot be approximated to the ratio O(n 1/3−ε) in polynomial time under reasonable complexity theoretic hypotheses. The ASBC algorithm is also analysed experimentally on a collection of realistic test cases from the Harwell–Boeing sparse matrix collection. The experimental results indicate that ASBC performs quite well in practice; the performance of the new algorithm always fell within the range of 1.53 and 6 times optimal on the given test sets.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/RZXIJ5BF/Juedes and Jones - 2012 - Coloring Jacobians revisited a new algorithm for .pdf}
}

@misc{juliadiffcontributorsJuliaDiffSparseDiffToolsjl2024,
  title = {JuliaDiff/SparseDiffTools.jl},
  author = {{JuliaDiff contributors}},
  year = {2024},
  month = oct,
  url = {https://github.com/JuliaDiff/SparseDiffTools.jl},
  urldate = {2024-11-05},
  abstract = {Fast jacobian computation through sparsity exploitation and matrix coloring},
  copyright = {MIT},
  keywords = {tracer}
}

@article{khanVectorForwardMode2015,
  title = {A vector forward mode of automatic differentiation for generalized derivative evaluation},
  author = {Khan, Kamil A. and Barton, Paul I.},
  year = {2015},
  month = nov,
  journal = {Optimization Methods and Software},
  volume = {30},
  number = {6},
  pages = {1185--1212},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556788.2015.1025400},
  url = {https://doi.org/10.1080/10556788.2015.1025400},
  urldate = {2024-05-08},
  abstract = {Numerical methods for non-smooth equation-solving and optimization often require generalized derivative information in the form of elements of the Clarke Jacobian or the B-subdifferential. It is shown here that piecewise differentiable functions are lexicographically smooth in the sense of Nesterov, and that lexicographic derivatives of these functions comprise a particular subset of both the B-subdifferential and the Clarke Jacobian. Several recently developed methods for generalized derivative evaluation of composite piecewise differentiable functions are shown to produce identical results, which are also lexicographic derivatives. A vector forward mode of automatic differentiation (AD) is presented for evaluation of these derivatives, generalizing established methods and combining their computational benefits. This forward AD mode may be applied to any finite composition of known smooth functions, piecewise differentiable functions such as the absolute value function, , and , and certain non-smooth functions which are not piecewise differentiable, such as the Euclidean norm. This forward AD mode may be implemented using operator overloading, does not require storage of a computational graph, and is computationally tractable relative to the cost of a function evaluation. An implementation in C is discussed.},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/93UJV76V/Khan and Barton - 2015 - A vector forward mode of automatic differentiation.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/PMNS2YN9/khan2015.pdf.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/XZPSDQTM/khan2015.pdf.pdf}
}

@misc{kolterDeepImplicitLayers2020,
  type = {NeurIPS tutorial},
  title = {Deep Implicit Layers - Neural ODEs, Deep Equilibirum Models, and Beyond},
  author = {Kolter, J. Zico and Duvenaud, David and Johnson, Matt},
  year = {2020},
  url = {http://implicit-layers-tutorial.org/},
  urldate = {2022-03-31},
  abstract = {This web page is the companion website to our NeurIPS 2020 tutorial, created by [Zico Kolter](http://zicokolter.com), [David Duvenaud](http://www.cs.toronto.edu/\textasciitilde duvenaud/), and [Matt Johnson](http://people.csail.mit.edu/mattjj/). The page constain notes to accompany our tutorial (all created via Colab notebooks, which you can experiment with as you like), as well as links to our video presentation...},
  langid = {english},
  keywords = {inferopt,semidone,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/CJWCSKW6/implicit-layers-tutorial.org.html}
}

@misc{kramerGradientsFunctionsLarge2024,
  title = {Gradients of Functions of Large Matrices},
  author = {Krämer, Nicholas and {Moreno-Muñoz}, Pablo and Roy, Hrittik and Hauberg, Søren},
  year = {2024},
  month = oct,
  number = {arXiv:2405.17277},
  eprint = {2405.17277},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.17277},
  url = {http://arxiv.org/abs/2405.17277},
  urldate = {2024-11-01},
  abstract = {Tuning scientific and probabilistic machine learning models \$-\$ for example, partial differential equations, Gaussian processes, or Bayesian neural networks \$-\$ often relies on evaluating functions of matrices whose size grows with the data set or the number of parameters. While the state-of-the-art for evaluating these quantities is almost always based on Lanczos and Arnoldi iterations, the present work is the first to explain how to differentiate these workhorses of numerical linear algebra efficiently. To get there, we derive previously unknown adjoint systems for Lanczos and Arnoldi iterations, implement them in JAX, and show that the resulting code can compete with Diffrax when it comes to differentiating PDEs, GPyTorch for selecting Gaussian process models and beats standard factorisation methods for calibrating Bayesian neural networks. All this is achieved without any problem-specific code optimisation. Find the code at https://github.com/pnkraemer/experiments-lanczos-adjoints and install the library with pip install matfree.},
  archiveprefix = {arXiv},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/48V29TA6/Krämer et al. - 2024 - Gradients of Functions of Large Matrices.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/7YQLPQDS/2405.html}
}

@article{kristensenTMBAutomaticDifferentiation2016,
  title = {TMB: Automatic Differentiation and Laplace Approximation},
  shorttitle = {TMB},
  author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper W. and Skaug, Hans and Bell, Bradley M.},
  year = {2016},
  month = apr,
  journal = {Journal of Statistical Software},
  volume = {70},
  pages = {1--21},
  issn = {1548-7660},
  doi = {10.18637/jss.v070.i05},
  url = {https://doi.org/10.18637/jss.v070.i05},
  urldate = {2024-05-07},
  abstract = {TMB is an open source R package that enables quick implementation of complex nonlinear random effects (latent variable) models in a manner similar to the established AD Model Builder package (ADMB, http://admb-project.org/; Fournier et al. 2011). In addition, it offers easy access to parallel computations. The user defines the joint likelihood for the data and the random effects as a C++ template function, while all the other operations are done in R; e.g., reading in the data. The package evaluates and maximizes the Laplace approximation of the marginal likelihood where the random effects are automatically integrated out. This approximation, and its derivatives, are obtained using automatic differentiation (up to order three) of the joint likelihood. The computations are designed to be fast for problems with many random effects (≈ 106 ) and parameters (≈ 103 ). Computation times using ADMB and TMB are compared on a suite of examples ranging from simple models to large spatial models where the random effects are a Gaussian random field. Speedups ranging from 1.5 to about 100 are obtained with increasing gains for large problems. The package and examples are available at http://tmb-project.org/.},
  copyright = {Copyright (c) 2016 Kasper Kristensen, Anders Nielsen, Casper W. Berg, Hans Skaug, Bradley M. Bell},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/8CDKUK8S/Kristensen et al. - 2016 - TMB Automatic Differentiation and Laplace Approxi.pdf}
}

@article{lawsonBasicLinearAlgebra1979,
  title = {Basic Linear Algebra Subprograms for Fortran Usage},
  author = {Lawson, C. L. and Hanson, R. J. and Kincaid, D. R. and Krogh, F. T.},
  year = {1979},
  month = sep,
  journal = {ACM Transactions on Mathematical Software},
  volume = {5},
  number = {3},
  pages = {308--323},
  issn = {0098-3500},
  doi = {10.1145/355841.355847},
  url = {https://dl.acm.org/doi/10.1145/355841.355847},
  urldate = {2024-05-08},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/HS33WACJ/Lawson et al. - 1979 - Basic Linear Algebra Subprograms for Fortran Usage.pdf}
}

@inproceedings{letschertExploitingSparsityAutomatic2012,
  title = {Exploiting Sparsity in Automatic Differentiation on Multicore Architectures},
  booktitle = {Recent Advances in Algorithmic Differentiation},
  author = {Letschert, Benjamin and Kulshreshtha, Kshitij and Walther, Andrea and Nguyen, Duc and Gebremedhin, Assefaw and Pothen, Alex},
  editor = {Forth, Shaun and Hovland, Paul and Phipps, Eric and Utke, Jean and Walther, Andrea},
  year = {2012},
  pages = {151--161},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-30023-3_14},
  abstract = {We discuss the design, implementation and performance of algorithms suitable for the efficient computation of sparse Jacobian and Hessian matrices using Automatic Differentiation via operator overloading on multicore architectures. The procedure for exploiting sparsity (for runtime and memory efficiency) in serial computation involves a number of steps. Using nonlinear optimization problems as test cases, we show that the algorithms involved in the various steps can be adapted to multithreaded computations.},
  isbn = {978-3-642-30023-3},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/MLTTSTIL/Letschert et al. - 2012 - Exploiting Sparsity in Automatic Differentiation o.pdf}
}

@inproceedings{liuBridgingDiscreteBackpropagation2023,
  title = {Bridging Discrete and Backpropagation: Straight-Through and Beyond},
  shorttitle = {Bridging Discrete and Backpropagation},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  author = {Liu, Liyuan and Dong, Chengyu and Liu, Xiaodong and Yu, Bin and Gao, Jianfeng},
  year = {2023},
  month = nov,
  url = {https://openreview.net/forum?id=mayAyPrhJI},
  urldate = {2023-12-12},
  abstract = {Backpropagation, the cornerstone of deep learning, is limited to computing gradients for continuous variables. This limitation poses challenges for problems involving discrete latent variables. To address this issue, we propose a novel approach to approximate the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose ReinMax, which achieves second-order accuracy by integrating Heun’s method, a second-order numerical method for solving ODEs. ReinMax does not require Hessian or other second-order derivatives, thus having negligible computation overheads. Extensive experimental results on various tasks demonstrate the superiority of ReinMax over the state of the art.},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/KQS4XHSQ/Liu et al. - 2023 - Bridging Discrete and Backpropagation Straight-Th.pdf}
}

@inproceedings{maclaurinAutogradEffortlessGradients2015,
  title = {Autograd: Effortless Gradients in Numpy},
  booktitle = {ICML 2015 AutoML workshop},
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P},
  year = {2015},
  url = {https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf},
  abstract = {Automatic differentiation can greatly speed up prototyping and implementation of machine learning models. However, most packages are implicitly domain-specific, requiring the use of a restricted mini-language for specifying functions. We introduce autograd, a package which differentiates standard Python and Numpy code, and can differentiate code containing while loops, branches, closures, classes and even its own gradient evaluations.},
  langid = {english},
  keywords = {⛔ No DOI found,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/5Y9FGJH8/Maclaurin et al. - Autograd Eﬀortless Gradients in Numpy.pdf}
}

@article{margossianReviewAutomaticDifferentiation2019,
  title = {A review of automatic differentiation and its efficient implementation},
  author = {Margossian, Charles C.},
  year = {2019},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1305},
  issn = {1942-4795},
  doi = {10.1002/widm.1305},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1305},
  urldate = {2023-03-12},
  abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation (AD) is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of AD, however, requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region-based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open and practical problems include the extension of current packages to provide more specialized routines, and finding optimal methods to perform higher-order differentiation. This article is categorized under: Algorithmic Development {$>$} Scalable Statistical Methods},
  langid = {english},
  keywords = {autodiff,done,inferopt,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/G6C6EX8U/Margossian_2019_A review of automatic differentiation and its efficient implementation.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/2JWFI24S/widm.html}
}

@article{metzGradientsAreNot2022,
  title = {Gradients are Not All You Need},
  author = {Metz, Luke and Freeman, C. Daniel and Schoenholz, Samuel S. and Kachman, Tal},
  year = {2022},
  month = jan,
  journal = {arXiv:2111.05803 [cs, stat]},
  eprint = {2111.05803},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2111.05803},
  urldate = {2022-02-20},
  abstract = {Differentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.},
  archiveprefix = {arXiv},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/A79FG9B8/Metz et al_2022_Gradients are Not All You Need.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/XVQ46ILC/2111.html}
}

@misc{mosesAutomatedDerivativeSparsity2023,
  title = {Automated Derivative Sparsity via Dead Code Elimination},
  author = {Moses, William S},
  year = {2023},
  url = {https://c.wsmoses.com/presentations/weuroad23.pdf},
  collaborator = {Mu, Kevin and Michel, Jessie and Kamil, Shoaib and Tatlock, Zachary and Jacobson, Alec},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/FEWWYIZG/Mu et al. - Automated Derivative Sparsity via Dead Code Elimination.pdf}
}

@inproceedings{mosesInsteadRewritingForeign2020,
  title = {Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Moses, William and Churavy, Valentin},
  year = {2020},
  volume = {33},
  pages = {12472--12485},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/9332c513ef44b682e9347822c2e457ac-Abstract.html},
  urldate = {2023-03-12},
  abstract = {Applying differentiable programming techniques and machine learning algorithms to foreign programs requires developers to either rewrite their code in a machine learning framework, or otherwise provide derivatives of the foreign code. This paper presents Enzyme, a high-performance automatic differentiation (AD) compiler plugin for the LLVM compiler framework capable of synthesizing gradients of statically analyzable programs expressed in the LLVM intermediate representation (IR). Enzyme synthesizes gradients for programs written in any language whose compiler targets LLVM IR including C, C++, Fortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD capabilities in these languages. Unlike traditional source-to-source and operator-overloading tools, Enzyme performs AD on optimized IR. On a machine-learning focused benchmark suite including Microsoft's ADBench, AD on optimized IR achieves a geometric mean speedup of 4.2 times over AD on IR before optimization allowing Enzyme to achieve state-of-the-art performance. Packaging Enzyme for PyTorch and TensorFlow provides convenient access to gradients of foreign code with state-of-the-art performance, enabling foreign code to be directly incorporated into existing machine learning workflows.},
  keywords = {inferopt,thesis,todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/HIZSQA2K/Moses_Churavy_2020_Instead of Rewriting Foreign Code for Machine Learning, Automatically.pdf}
}

@inproceedings{mosesReversemodeAutomaticDifferentiation2021,
  title = {Reverse-mode automatic differentiation and optimization of GPU kernels via Enzyme},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  author = {Moses, William S. and Churavy, Valentin and Paehler, Ludger and Hückelheim, Jan and Narayanan, Sri Hari Krishna and Schanen, Michel and Doerfert, Johannes},
  year = {2021},
  month = nov,
  series = {SC '21},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3458817.3476165},
  url = {https://doi.org/10.1145/3458817.3476165},
  urldate = {2022-08-07},
  abstract = {Computing derivatives is key to many algorithms in scientific computing and machine learning such as optimization, uncertainty quantification, and stability analysis. Enzyme is a LLVM compiler plugin that performs reverse-mode automatic differentiation (AD) and thus generates high performance gradients of programs in languages including C/C++, Fortran, Julia, and Rust. Prior to this work, Enzyme and other AD tools were not capable of generating gradients of GPU kernels. Our paper presents a combination of novel techniques that make Enzyme the first fully automatic reversemode AD tool to generate gradients of GPU kernels. Since unlike other tools Enzyme performs automatic differentiation within a general-purpose compiler, we are able to introduce several novel GPU and AD-specific optimizations. To show the generality and efficiency of our approach, we compute gradients of five GPU-based HPC applications, executed on NVIDIA and AMD GPUs. All benchmarks run within an order of magnitude of the original program's execution time. Without GPU and AD-specific optimizations, gradients of GPU kernels either fail to run from a lack of resources or have infeasible overhead. Finally, we demonstrate that increasing the problem size by either increasing the number of threads or increasing the work per thread, does not substantially impact the overhead from differentiation.},
  isbn = {978-1-4503-8442-1},
  keywords = {thesis,todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/YQ5LJCJ4/Moses et al_2021_Reverse-mode automatic differentiation and optimization of GPU kernels via.pdf}
}

@book{murphyProbabilisticMachineLearning2022,
  title = {Probabilistic Machine Learning: An Introduction},
  shorttitle = {Probabilistic Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2022},
  month = mar,
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {A detailed and up-to-date introduction to machine learning, presented through the unifying lens of probabilistic modeling and Bayesian decision theory.This book offers a detailed and up-to-date introduction to machine learning (including deep learning) through the unifying lens of probabilistic modeling and Bayesian decision theory. The book covers mathematical background (including linear algebra and optimization), basic supervised learning (including linear and logistic regression and deep neural networks), as well as more advanced topics (including transfer learning and unsupervised learning). End-of-chapter exercises allow students to apply what they have learned, and an appendix covers notation. Probabilistic Machine Learning grew out of the author’s 2012 book, Machine Learning: A Probabilistic Perspective. More than just a simple update, this is a completely new book that reflects the dramatic developments in the field since 2012, most notably deep learning. In addition, the new book is accompanied by online Python code, using libraries such as scikit-learn, JAX, PyTorch, and Tensorflow, which can be used to reproduce nearly all the figures; this code can be run inside a web browser using cloud-based notebooks, and provides a practical complement to the theoretical topics discussed in the book. This introductory text will be followed by a sequel that covers more advanced topics, taking the same probabilistic approach.},
  isbn = {978-0-262-04682-4},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/B33H86QG/Murphy - 2022 - Probabilistic Machine Learning An Introduction.pdf}
}

@article{narayananComparisonTwoGradient2017,
  title = {Comparison of two gradient computation methods in Python},
  author = {Narayanan, Sri Hari Krishna and Hovland, Paul and Kulshreshtha, Kshitij and Nagarkar, Devashri and MacIntyre, Kaitlyn and Wagner, Riley and Fu, Deqing},
  year = {2017},
  month = oct,
  url = {https://openreview.net/forum?id=r1K7G7GRW},
  urldate = {2024-10-04},
  abstract = {Gradient based optimization and machine learning applications require the computation of derivatives. For example, artificial neural networks (ANNs), a widely used learning system, use backpropagation to calculate the error contribution of each neuron after a batch of data is processed. Languages such as Python and R are are popular for machine learning. Therefore, there has been interest in the last few years to build tools for Python and R to compute derivatives.},
  langid = {english},
  keywords = {⛔ No DOI found,todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/ZEY2SIBF/Narayanan et al. - 2017 - Comparison of two gradient computation methods in .pdf}
}

@article{narayananSparseJacobianComputation2011,
  title = {Sparse Jacobian Computation Using ADIC2 and ColPack},
  author = {Narayanan, Sri Hari Krishna and Norris, Boyana and Hovland, Paul and Nguyen, Duc C. and Gebremedhin, Assefaw H.},
  year = {2011},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Proceedings of the International Conference on Computational Science, ICCS 2011},
  volume = {4},
  pages = {2115--2123},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2011.04.231},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050911002894},
  urldate = {2024-05-07},
  abstract = {Many scientific applications benefit from the accurate and efficient computation of derivatives. Automatically generating these derivative computations from an applications source code offers a competitive alternative to other approaches, such as less accurate numerical approximations or labor-intensive analytical implementations. ADIC2 is a source transformation tool for generating code for computing the derivatives (e.g., Jacobian or Hessian) of a function given the C or C++ implementation of that function. Often the Jacobian or Hessian is sparse and presents the opportunity to greatly reduce storage and computational requirements in the automatically generated derivative computation. ColPack is a tool that compresses structurally independent columns of the Jacobian and Hessian matrices through graph coloring approaches. In this paper, we describe the integration of ColPack coloring capabilities into ADIC2, enabling accurate and efficient sparse Jacobian computations. We present performance results for a case study of a simulated moving bed chromatography application. Overall, the computation of the Jacobian by integrating ADIC2 and ColPack is approximately 40\% faster than a comparable implementation that integrates ADOL-C and ColPack when the Jacobian is computed multiple times.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/98GE77J4/Narayanan et al. - 2011 - Sparse Jacobian Computation Using ADIC2 and ColPac.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/UVZKISNJ/S1877050911002894.html}
}

@misc{naumannMatrixFreeNewtonMethod2023,
  title = {A Matrix-Free Newton Method},
  author = {Naumann, Uwe},
  year = {2023},
  month = may,
  number = {arXiv:2305.01669},
  eprint = {2305.01669},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01669},
  url = {http://arxiv.org/abs/2305.01669},
  urldate = {2024-09-19},
  abstract = {A modification of Newton's method for solving systems of \$n\$ nonlinear equations is presented. The new matrix-free method relies on a given decomposition of the invertible Jacobian of the residual into invertible sparse local Jacobians according to the chain rule of differentiation. It is motivated in the context of local Jacobians with bandwidth \$2m+1\$ for \$m\textbackslash ll n\$. A reduction of the computational cost by \$\textbackslash mathcal\{O\}(\textbackslash frac\{n\}\{m\})\$ can be observed. Supporting run time measurements are presented for the tridiagonal case showing a reduction of the computational cost by \$\textbackslash mathcal\{O\}(n).\$ Generalization yields the combinatorial Matrix-Free Newton Step problem. We prove NP-completeness and we present algorithmic components for building methods for the approximate solution. Inspired by adjoint Algorithmic Differentiation, the new method shares several challenges for the latter including the DAG Reversal problem. Further challenges are due to combinatorial problems in sparse linear algebra such as Bandwidth or Directed Elimination Ordering.},
  archiveprefix = {arXiv},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/2W2PLU85/Naumann - 2023 - A Matrix-Free Newton Method.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/YTUQ8Z3B/2305.html}
}

@article{naumannOptimalJacobianAccumulation2008,
  title = {Optimal Jacobian accumulation is NP-complete},
  author = {Naumann, Uwe},
  year = {2008},
  month = apr,
  journal = {Mathematical Programming},
  volume = {112},
  number = {2},
  pages = {427--441},
  issn = {1436-4646},
  doi = {10.1007/s10107-006-0042-z},
  url = {https://doi.org/10.1007/s10107-006-0042-z},
  urldate = {2024-05-02},
  abstract = {We show that the problem of accumulating Jacobian matrices by using a minimal number of floating-point operations is NP-complete by reduction from Ensemble Computation. The proof makes use of the fact that, deviating from the state-of-the-art assumption, algebraic dependences can exist between the local partial derivatives. It follows immediately that the same problem for directional derivatives, adjoints, and higher derivatives is NP-complete, too.},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/XWFWM2VX/Naumann_2008_Optimal Jacobian accumulation is NP-complete.pdf}
}

@article{neidingerIntroductionAutomaticDifferentiation2010,
  title = {Introduction to Automatic Differentiation and MATLAB Object-Oriented Programming},
  author = {Neidinger, Richard D.},
  year = {2010},
  month = jan,
  journal = {SIAM Review},
  volume = {52},
  number = {3},
  pages = {545--563},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0036-1445},
  doi = {10.1137/080743627},
  url = {https://epubs.siam.org/doi/abs/10.1137/080743627},
  urldate = {2024-05-07},
  abstract = {Developing code for computing the first- and higher-order derivatives of a function by hand can be very time consuming and is prone to errors. Automatic differentiation has proven capable of producing derivative codes with very little effort on the part of the user. Automatic differentiation avoids the truncation errors characteristic of divided difference approximations. However, the derivative code produced by automatic differentiation can be significantly less efficient than one produced by hand. This shortcoming may be overcome by utilizing insight into the high-level structure of a computation. This paper focuses on how to take advantage of the fact that the number of variables passed between subroutines frequently is small compared with the number of variables with respect to which one wishes to differentiate. Such an "interface contraction," coupled with the associativity of the chain rule for differentiation, allows one to apply automatic differentiation in a more judicious fashion, resulting in much more efficient code for the computation of derivatives. A case study involving the ADIFOR (Automatic Differentiation of Fortran) tool and a program for maximizing a logistic-normal likelihood function developed from a problem in nutritional epidemiology is examined, and performance figures are presented.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/ZY72A2PK/Neidinger - 2010 - Introduction to Automatic Differentiation and MATL.pdf}
}

@article{newsamEstimationSparseJacobian1983,
  title = {Estimation of Sparse Jacobian Matrices},
  author = {Newsam, Garry N. and Ramsdell, John D.},
  year = {1983},
  month = sep,
  journal = {SIAM Journal on Algebraic Discrete Methods},
  volume = {4},
  number = {3},
  pages = {404--418},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0196-5212},
  doi = {10.1137/0604041},
  url = {https://epubs.siam.org/doi/abs/10.1137/0604041},
  urldate = {2024-05-08},
  abstract = {This paper is concerned with the efficient computation of sparse Jacobian matrices of nonlinear vector maps using automatic differentiation (AD). Specifically, we propose the use of a graph coloring technique, bicoloring, to exploit the sparsity of the Jacobian matrix J and thereby allow for the efficient determination of J using AD software. We analyze both a direct scheme and a substitution process. We discuss the results of numerical experiments indicating significant practical potential of this approach.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/2PKYLMVJ/newsam1983.pdf.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/BUJEE9NC/Newsam and Ramsdell - 1983 - Estimation of Sparse Jacobian Matrices.pdf}
}

@inproceedings{nobelAuto_diffAutomaticDifferentiation2020,
  title = {auto\_diff: an automatic differentiation package for Python},
  shorttitle = {auto\_diff},
  booktitle = {Proceedings of the 2020 Spring Simulation Conference},
  author = {Nobel, Parth},
  year = {2020},
  month = may,
  series = {SpringSim '20},
  pages = {1--12},
  publisher = {Society for Computer Simulation International},
  address = {San Diego, CA, USA},
  urldate = {2024-05-07},
  abstract = {We present auto\_diff, a package that performs automatic differentiation of numerical Python code. auto\_diff overrides Python's NumPy package's functions, augmenting them with seamless automatic differentiation capabilities. Notably, auto\_diff is non-intrusive, i.e., the code to be differentiated does not require auto\_diff-specific alterations. We illustrate auto\_diff on electronic devices, a circuit simulation, and a mechanical system simulation. In our evaluations so far, we found that running simulations with auto\_diff takes less than 4 times as long as simulations with hand-written differentiation code. We believe that auto\_diff, which was written after attempts to use existing automatic differentiation packages on our applications ran into difficulties, caters to an important need within the numerical Python community. We have attempted to write this paper in a tutorial style to make it accessible to those without prior background in automatic differentiation techniques and packages. We have released auto\_diff as open source on GitHub.},
  isbn = {978-1-71381-288-3},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/VZFXV3VH/Nobel - 2020 - auto_diff an automatic differentiation package fo.pdf}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen},
  year = {2006},
  month = jul,
  edition = {2nd edition},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-0-387-30303-1},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/L3J8F38I/Nocedal and Wright - 2006 - Numerical optimization.pdf}
}

@article{nocedalUpdatingQuasiNewtonMatrices1980,
  title = {Updating Quasi-Newton Matrices with Limited Storage},
  author = {Nocedal, Jorge},
  year = {1980},
  journal = {Mathematics of Computation},
  volume = {35},
  number = {151},
  eprint = {2006193},
  eprinttype = {jstor},
  pages = {773--782},
  publisher = {American Mathematical Society},
  issn = {0025-5718},
  doi = {10.2307/2006193},
  url = {https://www.jstor.org/stable/2006193},
  urldate = {2024-05-16},
  abstract = {We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last \$m\$ iterations, where \$m\$ is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest information. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several well-known methods.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/DFLKSKXG/Nocedal_1980_Updating Quasi-Newton Matrices with Limited Storage.pdf}
}

@misc{palNonlinearSolvejlHighPerformanceRobust2024,
  title = {NonlinearSolve.jl: High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia},
  shorttitle = {NonlinearSolve.jl},
  author = {Pal, Avik and Holtorf, Flemming and Larsson, Axel and Loman, Torkel and Utkarsh and Schäefer, Frank and Qu, Qingyu and Edelman, Alan and Rackauckas, Chris},
  year = {2024},
  month = mar,
  number = {arXiv:2403.16341},
  eprint = {2403.16341},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.16341},
  url = {http://arxiv.org/abs/2403.16341},
  urldate = {2024-11-05},
  abstract = {Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for complex system models remains a challenge. This paper presents NonlinearSolve.jl - a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language. NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving. Through rigorous comparison with established tools such as Sundials and MINPACK, NonlinearSolve.jl demonstrates unparalleled robustness and efficiency, achieving significant advancements in solving benchmark problems and challenging real-world applications. The capabilities of NonlinearSolve.jl unlock new potentials in modeling and simulation across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike.},
  archiveprefix = {arXiv},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/YW6G3ZFC/Pal et al. - 2024 - NonlinearSolve.jl High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/6GMPIYV6/2403.html}
}

@article{papamakariosNormalizingFlowsProbabilistic2021,
  title = {Normalizing Flows for Probabilistic Modeling and Inference},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {57},
  pages = {1--64},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v22/19-1028.html},
  urldate = {2022-10-11},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  keywords = {done,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/Z6758GD7/Papamakarios et al_2021_Normalizing Flows for Probabilistic Modeling and Inference.pdf}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  shorttitle = {PyTorch},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  urldate = {2024-05-06},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  keywords = {⛔ No DOI found,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/Z6BMB8KI/Paszke et al_2019_PyTorch.pdf}
}

@inproceedings{patwaryNewMultithreadedOrdering2011,
  title = {New Multithreaded Ordering and Coloring Algorithms for Multicore Architectures},
  booktitle = {Euro-Par 2011 Parallel Processing},
  author = {Patwary, Md. Mostofa Ali and Gebremedhin, Assefaw H. and Pothen, Alex},
  editor = {Jeannot, Emmanuel and Namyst, Raymond and Roman, Jean},
  year = {2011},
  pages = {250--262},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23397-5_24},
  abstract = {We present new multithreaded vertex ordering and distance-k graph coloring algorithms that are well-suited for multicore platforms. The vertex ordering techniques rely on various notions of “degree”, are known to be effective in reducing the number of colors used by a greedy coloring algorithm, and are generic enough to be applicable to contexts other than coloring. We employ approximate degree computation in the ordering algorithms and speculation and iteration in the coloring algorithms as our primary tools for breaking sequentiality and achieving effective parallelization. The algorithms have been implemented using OpenMP, and experiments conducted on Intel Nehalem and other multi-core machines using various types of graphs attest that the algorithms provide scalable runtime performance. The number of colors the algorithms use is often close to optimal. The techniques used for computing the ordering and coloring in parallel are applicable to other problems where there is an inherent ordering to the computations that needs to be relaxed for increasing concurrency.},
  isbn = {978-3-642-23397-5},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/Z2SYKKWH/Patwary et al. - 2011 - New Multithreaded Ordering and Coloring Algorithms.pdf}
}

@article{pearlmutterFastExactMultiplication1994,
  title = {Fast Exact Multiplication by the Hessian},
  author = {Pearlmutter, Barak A.},
  year = {1994},
  month = jan,
  journal = {Neural Computation},
  volume = {6},
  number = {1},
  pages = {147--160},
  issn = {0899-7667},
  doi = {10.1162/neco.1994.6.1.147},
  url = {https://ieeexplore.ieee.org/abstract/document/6796137},
  urldate = {2024-03-16},
  abstract = {Just storing the Hessian H (the matrix of second derivatives δ2E/δwiδwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rvf(w) = (δ/δr)f(w + rv)|r=0, note that Rv▽w = Hv and Rvw = v, and then apply Rv· to the equations used to compute ▽w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/QEM3B8NK/Pearlmutter_1994_Fast Exact Multiplication by the Hessian.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/SLGZT5PU/6796137.html}
}

@article{petraEfficientHessianComputation2018,
  title = {On efficient Hessian computation using the edge pushing algorithm in Julia},
  author = {Petra, C. G. and Qiang, F. and Lubin, M. and Huchette, J.},
  year = {2018},
  month = nov,
  journal = {Optimization Methods and Software},
  volume = {33},
  number = {4-6},
  pages = {1010--1029},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556788.2018.1480625},
  url = {https://doi.org/10.1080/10556788.2018.1480625},
  urldate = {2024-05-07},
  abstract = {Evaluating the Hessian matrix of second-order derivatives at a sequence of points can be costly when applying second-order methods for nonlinear optimization. In this work, we discuss our experiences implementing the recently proposed Edge Pushing (EP) method in Julia as an experimental replacement for the current colouring-based methods used by JuMP, an open-source algebraic modelling language. We propose an alternative data structure for sparse Hessians to avoid the use of hash tables and analyse the space and time complexity of EP method. In our benchmarks, we find that EP is very competitive in terms of both preprocessing time and Hessian evaluation time. We identify cases where EP closes the performance gap between JuMP's previous implementation and the implementation in AMPL, a commercial software package with similar functionality.},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/GTAUHU7V/Petra et al. - 2018 - On efficient Hessian computation using the edge pu.pdf}
}

@article{powellEstimationSparseHessian1979,
  title = {On the Estimation of Sparse Hessian Matrices},
  author = {Powell, M. J. D. and Toint, Ph. L.},
  year = {1979},
  month = dec,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {16},
  number = {6},
  pages = {1060--1074},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0036-1429},
  doi = {10.1137/0716078},
  url = {https://epubs.siam.org/doi/abs/10.1137/0716078},
  urldate = {2024-11-05},
  abstract = {Numerical optimization algorithms often require the (symmetric) matrix of second derivatives, \$\textbackslash nabla \textasciicircum 2 f( x )\$. If the Hessian matrix is large and sparse, then estimation by finite differences can be quite attractive since several schemes allow for estimation in much fewer than n gradient evaluations.The purpose of this paper is to analyze, from a combinatorial point of view, a class of methods known as substitution methods. We present a concise characterization of such methods in graph-theoretic terms. Using this characterization, we develop a complexity analysis of the general problem and derive a roundoff error bound on the Hessian approximation. Moreover, the graph model immediately reveals procedures to effect the substitution process optimally (i.e. using fewest possible substitutions given the differencing directions) in space proportional to the number of nonzeros in the Hessian matrix.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/7IQPUZPZ/Powell and Toint - 1979 - On the Estimation of Sparse Hessian Matrices.pdf}
}

@book{rasmussenGaussianProcessesMachine2006,
  title = {Gaussian processes for machine learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive computation and machine learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-18253-9},
  lccn = {QA274.4 .R37 2006},
  keywords = {done,thesis,tracer},
  annotation = {OCLC: ocm61285753},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/JR4L28LM/Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf}
}

@misc{revelsForwardModeAutomaticDifferentiation2016,
  title = {Forward-Mode Automatic Differentiation in Julia},
  author = {Revels, Jarrett and Lubin, Miles and Papamarkou, Theodore},
  year = {2016},
  month = jul,
  number = {arXiv:1607.07892},
  eprint = {1607.07892},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.07892},
  url = {http://arxiv.org/abs/1607.07892},
  urldate = {2022-08-11},
  abstract = {We present ForwardDiff, a Julia package for forward-mode automatic differentiation (AD) featuring performance competitive with low-level languages like C++. Unlike recently developed AD tools in other popular high-level languages such as Python and MATLAB, ForwardDiff takes advantage of just-in-time (JIT) compilation to transparently recompile AD-unaware user code, enabling efficient support for higher-order differentiation and differentiation using custom number types (including complex numbers). For gradient and Jacobian calculations, ForwardDiff provides a variant of vector-forward mode that avoids expensive heap allocation and makes better use of memory bandwidth than traditional vector mode. In our numerical experiments, we demonstrate that for nontrivially large dimensions, ForwardDiff's gradient computations can be faster than a reverse-mode implementation from the Python-based autograd package. We also illustrate how ForwardDiff is used effectively within JuMP, a modeling language for optimization. According to our usage statistics, 41 unique repositories on GitHub depend on ForwardDiff, with users from diverse fields such as astronomy, optimization, finite element analysis, and statistics. This document is an extended abstract that has been accepted for presentation at the AD2016 7th International Conference on Algorithmic Differentiation.},
  archiveprefix = {arXiv},
  keywords = {done,thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/ZXUSBVZK/Revels et al_2016_Forward-Mode Automatic Differentiation in Julia.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/SP7X68BB/1607.html}
}

@book{robertMonteCarloStatistical2005,
  title = {Monte Carlo Statistical Methods},
  author = {Robert, Christian and Casella, George},
  year = {2005},
  month = aug,
  publisher = {Springer New York},
  abstract = {Monte Carlo statistical methods, particularly those based on Markov chains, are now an essential component of the standard set of techniques used by statisticians. This new edition has been revised towards a coherent and flowing coverage of these simulation techniques, with incorporation of the most recent developments in the field. In particular, the introductory coverage of random variable generation has been totally revised, with many concepts being unified through a fundamental theorem of simulation  There are five completely new chapters that cover Monte Carlo control, reversible jump, slice sampling, sequential Monte Carlo, and perfect sampling. There is a more in-depth coverage of Gibbs sampling, which is now contained in three consecutive chapters. The development of Gibbs sampling starts with slice sampling and its connection with the fundamental theorem of simulation, and builds up to two-stage Gibbs sampling and its theoretical properties. A third chapter covers the multi-stage Gibbs sampler and its variety of applications. Lastly, chapters from the previous edition have been revised towards easier access, with the examples getting more detailed coverage.  This textbook is intended for a second year graduate course, but will also be useful to someone who either wants to apply simulation techniques for the resolution of practical problems or wishes to grasp the fundamental principles behind those methods. The authors do not assume familiarity with Monte Carlo techniques (such as random variable generation), with computer programming, or with any Markov chain theory (the necessary concepts are developed in Chapter 6). A solutions manual, which covers approximately 40\% of the problems, is available for instructors who require the book for a course.  Christian P. Robert is Professor of Statistics in the Applied Mathematics Department at Université Paris Dauphine, France. He is also Head of the Statistics Laboratory at the Center for Research in Economics and Statistics (CREST) of the National Institute for Statistics and Economic Studies (INSEE) in Paris, and Adjunct Professor at Ecole Polytechnique. He has written three other books, including The Bayesian Choice, Second Edition, Springer 2001. He also edited Discretization and MCMC Convergence Assessment, Springer 1998. He has served as associate editor for the Annals of Statistics and the Journal of the American Statistical Association. He is a fellow of the Institute of Mathematical Statistics, and a winner of the Young Statistician Award of the Societié de Statistique de Paris in 1995.  George Casella is Distinguished Professor and Chair, Department of Statistics, University of Florida. He has served as the Theory and Methods Editor of the Journal of the American Statistical Association and Executive Editor of Statistical Science. He has authored three other textbooks: Statistical Inference, Second Edition, 2001, with Roger L. Berger; Theory of Point Estimation, 1998, with Erich Lehmann; and Variance Components, 1992, with Shayle R. Searle and Charles E. McCulloch. He is a fellow of the Institute of Mathematical Statistics and the American Statistical Association, and an elected fellow of the International Statistical Institute.},
  googlebooks = {HfhGAxn5GugC},
  isbn = {978-0-387-21239-5},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/DWEU8V2Q/Robert_Casella_2005_Monte Carlo Statistical Methods.pdf}
}

@article{rostaingAutomaticDifferentiationOdyssee1993,
  title = {Automatic differentiation in Odyssée},
  author = {Rostaing, Nicole and Dalmas, StéPhane and Galligo, André},
  year = {1993},
  month = jan,
  journal = {Tellus A: Dynamic Meteorology and Oceanography},
  volume = {45},
  number = {5},
  pages = {558--568},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.3402/tellusa.v45i5.15060},
  url = {https://doi.org/10.3402/tellusa.v45i5.15060},
  urldate = {2024-05-08},
  abstract = {This paper describes the design of Odyssée, a system for fortran programs manipulations and its application to automatic differentiation. The Odyssée system manipulates fortran programs as symbolic objects. It is an open system built as a toolkit, written in a high-level programming language adapted to this purpose. The use of a variational method to perform data assimilation requires the computation of the gradient of a cost function represented by a large-size fortran program. The usual drawback in the reverse automatic differentiation method is the storage requirement. The Odyssée system allows one to implement storage/recomputation strategies in order to fit the needed compromizes. We present the implementation of the strategy used in the weather forecasting arpege/ifs project to produce the adjoint code from the code representing the numerical model. Odyssée produces the same code as the hand-written adjoint code for thearpege/ifs project.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/F7ISWPTU/Rostaing et al. - 1993 - Automatic differentiation in Odyssée.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/T4X35INY/657559edb63b327529c37f8424fe1108.pdf.pdf}
}

@article{saadGMRESGeneralizedMinimal1986,
  title = {GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems},
  shorttitle = {GMRES},
  author = {Saad, Youcef and Schultz, Martin H.},
  year = {1986},
  month = jul,
  journal = {SIAM Journal on Scientific and Statistical Computing},
  volume = {7},
  number = {3},
  pages = {856--869},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {0196-5204},
  doi = {10.1137/0907058},
  url = {https://epubs.siam.org/doi/abs/10.1137/0907058},
  urldate = {2022-08-13},
  abstract = {We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from the Arnoldi process for constructing an \$l\_2 \$-orthogonal basis of Krylov subspaces. It can be considered as a generalization of Paige and Saunders’ MINRES algorithm and is theoretically equivalent to the Generalized Conjugate Residual (GCR) method and to ORTHODIR. The new algorithm presents several advantages over GCR and ORTHODIR.},
  keywords = {thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/DLTBKIA7/Saad_Schultz_1986_GMRES.pdf}
}

@misc{sapienzaDifferentiableProgrammingDifferential2024,
  title = {Differentiable Programming for Differential Equations: A Review},
  shorttitle = {Differentiable Programming for Differential Equations},
  author = {Sapienza, Facundo and Bolibar, Jordi and Schäfer, Frank and Groenke, Brian and Pal, Avik and Boussange, Victor and Heimbach, Patrick and Hooker, Giles and Pérez, Fernando and Persson, Per-Olof and Rackauckas, Christopher},
  year = {2024},
  month = jun,
  number = {arXiv:2406.09699},
  eprint = {2406.09699},
  primaryclass = {physics, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.09699},
  urldate = {2024-06-17},
  abstract = {The differentiable programming paradigm is a cornerstone of modern scientific computing. It refers to numerical methods for computing the gradient of a numerical model's output. Many scientific models are based on differential equations, where differentiable programming plays a crucial role in calculating model sensitivities, inverting model parameters, and training hybrid models that combine differential equations with data-driven approaches. Furthermore, recognizing the strong synergies between inverse methods and machine learning offers the opportunity to establish a coherent framework applicable to both fields. Differentiating functions based on the numerical solution of differential equations is non-trivial. Numerous methods based on a wide variety of paradigms have been proposed in the literature, each with pros and cons specific to the type of problem investigated. Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equations. We first discuss the importance of gradients of solutions of differential equations in a variety of scientific domains. Second, we lay out the mathematical foundations of the various approaches and compare them with each other. Third, we cover the computational considerations and explore the solutions available in modern scientific software. Last but not least, we provide best-practices and recommendations for practitioners. We hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.},
  archiveprefix = {arXiv},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/5KILVDWR/Sapienza et al. - 2024 - Differentiable Programming for Differential Equati.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/VZIAPMK7/2406.html}
}

@misc{schaferAbstractDifferentiationjlBackendAgnosticDifferentiable2021,
  title = {AbstractDifferentiation{{.jl}}: Backend-Agnostic Differentiable Programming in Julia},
  shorttitle = {AbstractDifferentiation.jl},
  author = {Schäfer, Frank and Tarek, Mohamed and White, Lyndon and Rackauckas, Chris},
  year = {2021},
  month = sep,
  eprint = {2109.12449},
  url = {http://arxiv.org/abs/2109.12449},
  urldate = {2022-02-02},
  abstract = {No single Automatic Differentiation (AD) system is the optimal choice for all problems. This means informed selection of an AD system and combinations can be a problem-specific variable that can greatly impact performance. In the Julia programming language, the major AD systems target the same input and thus in theory can compose. Hitherto, switching between AD packages in the Julia Language required end-users to familiarize themselves with the user-facing API of the respective packages. Furthermore, implementing a new, usable AD package required AD package developers to write boilerplate code to define convenience API functions for end-users. As a response to these issues, we present AbstractDifferentiation.jl for the automatized generation of an extensive, unified, user-facing API for any AD package. By splitting the complexity between AD users and AD developers, AD package developers only need to implement one or two primitive definitions to support various utilities for AD users like Jacobians, Hessians and lazy product operators from native primitives such as pullbacks or pushforwards, thus removing tedious -- but so far inevitable -- boilerplate code, and enabling the easy switching and composing between AD implementations for end-users.},
  archiveprefix = {arXiv},
  keywords = {thesis,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/AIAMD9ZK/Schafer et al_2021_AbstractDifferentiation.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/KQU49THD/2109.html}
}

@misc{schechtmanGradientsLimitDefinable2024,
  title = {The gradient's limit of a definable family of functions is a conservative set-valued field},
  author = {Schechtman, Sholom},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08272},
  eprint = {2402.08272},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.08272},
  url = {http://arxiv.org/abs/2402.08272},
  urldate = {2024-03-02},
  abstract = {It is well-known that the convergence of a family of smooth functions does not imply the convergence of its gradients. In this work, we show that if the family is definable in an o-minimal structure (for instance semialgebraic, subanalytic, or any composition of the previous with exp, log), then the gradient's limit is a conservative set-valued field in the sense introduced by Bolte and Pauwels. Immediate implications of this result on convergence guarantees of smoothing methods are discussed. Finally, a more general result is established, where the functions in the original family might be merely Lipschitz continuous, vector-valued and the gradients are replaced by their Clarke's Jacobians or an arbitrary definable conservative mapping.},
  archiveprefix = {arXiv},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/FCQCH4HF/Schechtman - 2024 - The gradient's limit of a definable family of func.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/ZRGPH63N/2402.html}
}

@article{schmidtTinyADAutomaticDifferentiation2022,
  title = {TinyAD: Automatic Differentiation in Geometry Processing Made Simple},
  shorttitle = {TinyAD},
  author = {Schmidt, P. and Born, J. and Bommes, D. and Campen, M. and Kobbelt, L.},
  year = {2022},
  journal = {Computer Graphics Forum},
  volume = {41},
  number = {5},
  pages = {113--124},
  issn = {1467-8659},
  doi = {10.1111/cgf.14607},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14607},
  urldate = {2024-04-26},
  abstract = {Non-linear optimization is essential to many areas of geometry processing research. However, when experimenting with different problem formulations or when prototyping new algorithms, a major practical obstacle is the need to figure out derivatives of objective functions, especially when second-order derivatives are required. Deriving and manually implementing gradients and Hessians is both time-consuming and error-prone. Automatic differentiation techniques address this problem, but can introduce a diverse set of obstacles themselves, e.g. limiting the set of supported language features, imposing restrictions on a program's control flow, incurring a significant run time overhead, or making it hard to exploit sparsity patterns common in geometry processing. We show that for many geometric problems, in particular on meshes, the simplest form of forward-mode automatic differentiation is not only the most flexible, but also actually the most efficient choice. We introduce TinyAD: a lightweight C++ library that automatically computes gradients and Hessians, in particular of sparse problems, by differentiating small (tiny) sub-problems. Its simplicity enables easy integration; no restrictions on, e.g., looping and branching are imposed. TinyAD provides the basic ingredients to quickly implement first and second order Newton-style solvers, allowing for flexible adjustment of both problem formulations and solver details. By showcasing compact implementations of methods from parametrization, deformation, and direction field design, we demonstrate how TinyAD lowers the barrier to exploring non-linear optimization techniques. This enables not only fast prototyping of new research ideas, but also improves replicability of existing algorithms in geometry processing. TinyAD is available to the community as an open source library.},
  copyright = {© 2022 The Author(s) Computer Graphics Forum © 2022 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/8G89HWYS/Schmidt et al_2022_TinyAD.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/HAHUKZ77/cgf.html}
}

@misc{schubertMfschubertSparsejac2024,
  title = {mfschubert/sparsejac},
  author = {Schubert, Martin},
  year = {2024},
  month = sep,
  url = {https://github.com/mfschubert/sparsejac},
  urldate = {2024-11-05},
  abstract = {Efficient forward- and reverse-mode sparse Jacobians using Jax},
  copyright = {MIT},
  keywords = {tracer}
}

@misc{shinAcceleratingOptimalPower2024,
  title = {Accelerating Optimal Power Flow with GPUs: SIMD Abstraction of Nonlinear Programs and Condensed-Space Interior-Point Methods},
  shorttitle = {Accelerating Optimal Power Flow with GPUs},
  author = {Shin, Sungho and Pacaud, François and Anitescu, Mihai},
  year = {2024},
  month = feb,
  number = {arXiv:2307.16830},
  eprint = {2307.16830},
  primaryclass = {cs, math},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.16830},
  urldate = {2024-06-24},
  abstract = {This paper introduces a framework for solving alternating current optimal power flow (ACOPF) problems using graphics processing units (GPUs). While GPUs have demonstrated remarkable performance in various computing domains, their application in ACOPF has been limited due to challenges associated with porting sparse automatic differentiation (AD) and sparse linear solver routines to GPUs. We address these issues with two key strategies. First, we utilize a single-instruction, multiple-data abstraction of nonlinear programs. This approach enables the specification of model equations while preserving their parallelizable structure and, in turn, facilitates the parallel AD implementation. Second, we employ a condensed-space interior-point method (IPM) with an inequality relaxation. This technique involves condensing the Karush--Kuhn--Tucker (KKT) system into a positive definite system. This strategy offers the key advantage of being able to factorize the KKT matrix without numerical pivoting, which has hampered the parallelization of the IPM algorithm. By combining these strategies, we can perform the majority of operations on GPUs while keeping the data residing in the device memory only. Comprehensive numerical benchmark results showcase the advantage of our approach. Remarkably, our implementations -- MadNLP.jl and ExaModels.jl -- running on NVIDIA GPUs achieve an order of magnitude speedup compared with state-of-the-art tools running on contemporary CPUs.},
  archiveprefix = {arXiv},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/JWSGDVIV/Shin et al. - 2024 - Accelerating Optimal Power Flow with GPUs SIMD Ab.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/AWEZLDK8/2307.html}
}

@misc{simpsonGarconPasComme2024,
  title = {Un garçon pas comme les autres (Bayes) - An unexpected detour into partially symbolic, sparsity-expoiting autodiff; or Lord won’t you buy me a Laplace approximation},
  author = {Simpson, Dan},
  year = {2024},
  month = may,
  journal = {Un garçon pas comme les autres (Bayes)},
  url = {https://dansblog.netlify.app/posts/2024-05-08-laplace/laplace},
  urldate = {2024-10-27},
  abstract = {Exploiting linearity and sparisty to speed up JAX Hessians and slowly ruin my life.},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/J48KAJ3W/laplace.html}
}

@unpublished{SparseDifferentiationDead2024,
  title = {Sparse Differentiation as Dead Code Elimination},
  year = {2024},
  keywords = {semidone,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/4RSBHGFV/2024_Sparse Differentiation as Dead Code Elimination.pdf}
}

@misc{srajerBenchmarkSelectedAlgorithmic2018,
  title = {A Benchmark of Selected Algorithmic Differentiation Tools on Some Problems in Computer Vision and Machine Learning},
  author = {Šrajer, Filip and Kukelova, Zuzana and Fitzgibbon, Andrew},
  year = {2018},
  month = jul,
  number = {arXiv:1807.10129},
  eprint = {1807.10129},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.10129},
  url = {http://arxiv.org/abs/1807.10129},
  urldate = {2024-04-17},
  abstract = {Algorithmic differentiation (AD) allows exact computation of derivatives given only an implementation of an objective function. Although many AD tools are available, a proper and efficient implementation of AD methods is not straightforward. The existing tools are often too different to allow for a general test suite. In this paper, we compare fifteen ways of computing derivatives including eleven automatic differentiation tools implementing various methods and written in various languages (C++, F\#, MATLAB, Julia and Python), two symbolic differentiation tools, finite differences, and hand-derived computation. We look at three objective functions from computer vision and machine learning. These objectives are for the most part simple, in the sense that no iterative loops are involved, and conditional statements are encapsulated in functions such as \{\textbackslash tt abs\} or \{\textbackslash tt logsumexp\}. However, it is important for the success of algorithmic differentiation that such `simple' objective functions are handled efficiently, as so many problems in computer vision and machine learning are of this form. Of course, our results depend on programmer skill, and familiarity with the tools. However, we contend that this paper presents an important datapoint: a skilled programmer devoting roughly a week to each tool produced the timings we present. We have made our implementations available as open source to allow the community to replicate and update these benchmarks.},
  archiveprefix = {arXiv},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/8H9QF7WQ/Šrajer et al. - 2018 - A Benchmark of Selected Algorithmic Differentiatio.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/MQ8QMPZ6/1807.html}
}

@inproceedings{tadjouddineSparseJacobianComputation1998,
  title = {Sparse Jacobian Computation in Automatic Differentiation by Static Program Analysis},
  booktitle = {Static Analysis},
  author = {Tadjouddine, M. and Eyssette, F. and Faure, C.},
  editor = {Levi, Giorgio},
  year = {1998},
  pages = {311--326},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-49727-7_19},
  abstract = {A major dificulty in quickly computing Jacobians by Automatic Differentiation is to deal with the nonzero structures of sparse matrices. We propose to detect the sparsity structure of Jacobians by static program analysis. The method consists in traversing the data dependence graph extended with the control-flow of the program and computing relations between array regions. Then, we safely extract informations about the dependences from program inputs to program outputs. The generation of the derived program uses these informations to produce a better result. We eventually, introduce the Automatic Differentiation tool Odyssée and present some benchmark tests.},
  isbn = {978-3-540-49727-1},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/TYN3UPB9/Tadjouddine et al. - 1998 - Sparse Jacobian Computation in Automatic Different.pdf}
}

@article{toivanenImplementationSparseForward2011,
  title = {Implementation of sparse forward mode automatic differentiation with application to electromagnetic shape optimization},
  author = {Toivanen, Jukka I. and Mäkinen, Raino A. E.},
  year = {2011},
  month = oct,
  journal = {Optimization Methods and Software},
  volume = {26},
  number = {4-5},
  pages = {601--616},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556781003642305},
  url = {https://doi.org/10.1080/10556781003642305},
  urldate = {2024-05-07},
  abstract = {In this paper, we present the details of a simple lightweight implementation of the so-called sparse forward mode automatic differentiation (AD) in the C++programming language. Our implementation and the well-known ADOL-C tool (which utilizes taping and compression techniques) are used to compute Jacobian matrices of two nonlinear systems of equations from the MINPACK-2 test problem collection. Timings of the computations are presented and discussed. Moreover, we perform the shape sensitivity analysis of a time-harmonic Maxwell equation solver using our implementation and the tapeless mode of ADOL-C, which implements the dense forward mode AD. It is shown that the use of the sparse forward mode can save computation time even though the total number of independent variables in this example is quite small. Finally, numerical solution of an electromagnetic shape optimization problem is presented.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/62BHSMEU/Toivanen and Mäkinen - 2011 - Implementation of sparse forward mode automatic di.pdf}
}

@article{vandemeentIntroductionProbabilisticProgramming2021,
  title = {An Introduction to Probabilistic Programming},
  author = {{van de Meent}, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  year = {2021},
  month = oct,
  journal = {arXiv:1809.10756 [cs, stat]},
  eprint = {1809.10756},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.10756},
  urldate = {2021-10-21},
  abstract = {This book is a graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning is a foundational computation central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a first-order probabilistic programming language (PPL) whose programs correspond to graphical models with a known, finite, set of random variables. In the context of this PPL we introduce fundamental inference algorithms and describe how they can be implemented. We then turn to higher-order probabilistic programming languages. Programs in such languages can define models with dynamic computation graphs, which may not instantiate the same set of random variables in each execution. Inference requires methods that generate samples by repeatedly evaluating the program. Foundational algorithms for this kind of language are discussed in the context of an interface between program executions and an inference controller. Finally we consider the intersection of probabilistic and differentiable programming. We begin with a discussion of automatic differentiation, and how it can be used to implement efficient inference methods based on Hamiltonian Monte Carlo. We then discuss gradient-based maximum likelihood estimation in programs that are parameterized using neural networks, how to amortize inference using by learning neural approximations to the program posterior, and how language features impact the design of deep probabilistic programming systems.},
  archiveprefix = {arXiv},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/PV4IEB9M/van de Meent et al_2021_An Introduction to Probabilistic Programming.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/NA38LXRP/1809.html}
}

@phdthesis{varnikExploitationStructuralSparsity2011,
  title = {Exploitation of Structural Sparsity in Algorithmic Differentiation},
  author = {Varnik, Ebadollah},
  year = {2011},
  address = {Germany},
  url = {https://publications.rwth-aachen.de/record/82673/files/82673.pdf},
  urldate = {2024-05-08},
  school = {RWTH Aachen},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/XV6ML2KT/82673.pdf}
}

@inproceedings{varnikFastConservativeEstimation2011,
  title = {Fast Conservative Estimation of Hessian Sparsity},
  booktitle = {Fifth SIAM Workshop on Combinatorial Scientific Computing},
  author = {Varnik, Ebadollah and Razik, Lukas and Mosenkis, Viktor and Naumann, Uwe},
  year = {2011-05-19/2011-05-21},
  pages = {18},
  address = {Darmstadt, Germany},
  url = {http://ftp.informatik.rwth-aachen.de/Publications/AIB/2011/2011-09.pdf#page=21},
  keywords = {⛔ No DOI found,todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/LWVAHPI3/Varnik et al. - 2011 - Fast Conservative Estimation of Hessian Spar-sity.pdf}
}

@article{virtanenSciPy10Fundamental2020,
  title = {SciPy 1.0: fundamental algorithms for scientific computing in Python},
  shorttitle = {SciPy 1.0},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and {van Mulbregt}, Paul},
  year = {2020},
  month = mar,
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  pages = {261--272},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0686-2},
  url = {https://www.nature.com/articles/s41592-019-0686-2},
  urldate = {2024-11-05},
  abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/SZ7FJJVS/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific computing in Python.pdf}
}

@article{waltherComputingSparseHessians2008,
  title = {Computing sparse Hessians with automatic differentiation},
  author = {Walther, Andrea},
  year = {2008},
  month = jan,
  journal = {ACM Transactions on Mathematical Software},
  volume = {34},
  number = {1},
  pages = {3:1--3:15},
  issn = {0098-3500},
  doi = {10.1145/1322436.1322439},
  url = {https://dl.acm.org/doi/10.1145/1322436.1322439},
  urldate = {2024-03-27},
  abstract = {A new approach for computing a sparsity pattern for a Hessian is presented: nonlinearity information is propagated through the function evaluation yielding the nonzero structure. A complexity analysis of the proposed algorithm is given. Once the sparsity pattern is available, coloring algorithms can be applied to compute a seed matrix. To evaluate the product of the Hessian and the seed matrix, a vector version for evaluating second order adjoints is analysed. New drivers of ADOL-C are provided implementing the presented algorithms. Runtime analyses are given for some problems of the CUTE collection.},
  keywords = {done,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/788MQVJV/Walther - 2008 - Computing sparse Hessians with automatic different.pdf}
}

@inproceedings{waltherEfficientComputationSparsity2012,
  title = {On the Efficient Computation of Sparsity Patterns for Hessians},
  booktitle = {Recent Advances in Algorithmic Differentiation},
  author = {Walther, Andrea},
  editor = {Forth, Shaun and Hovland, Paul and Phipps, Eric and Utke, Jean and Walther, Andrea},
  year = {2012},
  pages = {139--149},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-30023-3_13},
  abstract = {The exploitation of sparsity forms an important ingredient for the efficient solution of large-scale problems. For this purpose, this paper discusses two algorithms to detect the sparsity pattern of Hessians: An approach for the computation of exact sparsity patterns and a second one for the overestimation of sparsity patterns. For both algorithms, corresponding complexity results are stated. Subsequently, new data structures and set operations are presented yielding a new complexity result together with an alternative implementation of the exact approach. For several test problems, the obtained runtimes confirm the new theoretical result, i.e., a significant reduction in the runtime needed by the exact approach. A comparison with the runtime required for the overestimation of the sparsity pattern is included together with a corresponding discussion. Finally, possible directions for future research are stated.},
  isbn = {978-3-642-30023-3},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/RIXIRZ8H/Walther - 2012 - On the Efficient Computation of Sparsity Patterns .pdf}
}

@article{waltherGettingStartedADOLC2009,
  title = {Getting Started with ADOL-C},
  author = {Walther, Andrea},
  year = {2009},
  journal = {DROPS-IDN/v2/document/10.4230/DagSemProc.09061.10},
  publisher = {Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
  doi = {10.4230/DagSemProc.09061.10},
  url = {https://drops.dagstuhl.de/entities/document/10.4230/DagSemProc.09061.10},
  urldate = {2024-06-05},
  abstract = {The C++ package ADOL-C described in this paper facilitates the evaluation of first and higher derivatives of vector functions that are defined by computer programs written in C or C++. The numerical values of derivative vectors are obtained free of truncation errors at mostly a small multiple of the run time and a fix small multiple random access memory required by the given function evaluation program. Derivative matrices are obtained by columns, by rows or in sparse format. This tutorial describes the source code modification required for the application of ADOL-C, the most frequently used drivers to evaluate derivatives and some recent developments.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  keywords = {done,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/MAV7WCRX/Walther - 2009 - Getting Started with ADOL-C.pdf}
}

@article{wangCapitalizingLiveVariables2016,
  title = {Capitalizing on live variables: new algorithms for efficient Hessian computation via automatic differentiation},
  shorttitle = {Capitalizing on live variables},
  author = {Wang, Mu and Gebremedhin, Assefaw and Pothen, Alex},
  year = {2016},
  month = dec,
  journal = {Mathematical Programming Computation},
  volume = {8},
  number = {4},
  pages = {393--433},
  issn = {1867-2957},
  doi = {10.1007/s12532-016-0100-3},
  url = {https://doi.org/10.1007/s12532-016-0100-3},
  urldate = {2024-05-07},
  abstract = {We revisit an algorithm [called Edge Pushing (EP)] for computing Hessians using Automatic Differentiation (AD) recently proposed by Gower and Mello (Optim Methods Softw 27(2): 233–249, 2012). Here we give a new, simpler derivation for the EP algorithm based on the notion of live variables from data-flow analysis in compiler theory and redesign the algorithm with close attention to general applicability and performance. We call this algorithm Livarh and develop an extension of Livarh that incorporates preaccumulation to further reduce execution time—the resulting algorithm is called Livarhacc. We engineer robust implementations for both algorithms Livarh and Livarhacc within ADOL-C, a widely-used operator overloading based AD software tool. Rigorous complexity analyses for the algorithms are provided, and the performance of the algorithms is evaluated using a mesh optimization application and several kinds of synthetic functions as testbeds. The results show that the new algorithms outperform state-of-the-art sparse methods (based on sparsity pattern detection, coloring, compressed matrix evaluation, and recovery) in some cases by orders of magnitude. We have made our implementation available online as open-source software and it will be included in a future release of ADOL-C.},
  langid = {english},
  keywords = {todo,tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/RTFZ7IPP/Wang et al. - 2016 - Capitalizing on live variables new algorithms for.pdf}
}

@incollection{wangEdgePushingEquivalent2016,
  title = {Edge Pushing is Equivalent to Vertex Elimination for Computing Hessians},
  booktitle = {2016 Proceedings of the SIAM Workshop on Combinatorial Scientific Computing (CSC)},
  author = {Wang, Mu and Pothen, Alex and Hovland, Paul},
  year = {2016},
  month = jan,
  series = {Proceedings},
  pages = {102--111},
  publisher = {Society for Industrial and Applied Mathematics},
  doi = {10.1137/1.9781611974690.ch11},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974690.ch11},
  urldate = {2024-05-07},
  abstract = {We prove the equivalence of two different Hessian evaluation algorithms in AD. The first is the Edge Pushing algorithm of Gower and Mello, which may be viewed as a second order Reverse mode algorithm for computing the Hessian. In earlier work, we have derived the Edge Pushing algorithm by exploiting a Reverse mode invariant based on the concept of live variables in compiler theory. The second algorithm is based on eliminating vertices in a computational graph of the gradient, in which intermediate variables are successively eliminated from the graph, and the weights of the edges are updated suitably. We prove that if the vertices are eliminated in a reverse topological order while preserving symmetry in the computational graph of the gradient, then the Vertex Elimination algorithm and the Edge Pushing algorithm perform identical computations. In this sense, the two algorithms are equivalent. This insight that unifies two seemingly disparate approaches to Hessian computations could lead to improved algorithms and implementations for computing Hessians.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/84BD2KSS/Wang et al. - 2016 - Edge Pushing is Equivalent to Vertex Elimination f.pdf}
}

@article{weinsteinAlgorithm984ADiGator2017,
  title = {Algorithm 984: ADiGator, a Toolbox for the Algorithmic Differentiation of Mathematical Functions in MATLAB Using Source Transformation via Operator Overloading},
  shorttitle = {Algorithm 984},
  author = {Weinstein, Matthew J. and Rao, Anil V.},
  year = {2017},
  month = aug,
  journal = {ACM Transactions on Mathematical Software},
  volume = {44},
  number = {2},
  pages = {21:1--21:25},
  issn = {0098-3500},
  doi = {10.1145/3104990},
  url = {https://dl.acm.org/doi/10.1145/3104990},
  urldate = {2024-05-07},
  abstract = {A toolbox called ADiGator is described for algorithmically differentiating mathematical functions in MATLAB. ADiGator performs source transformation via operator overloading using forward mode algorithmic differentiation and produces a file that can be evaluated to obtain the derivative of the original function at a numeric value of the input. A convenient by-product of the file generation is the sparsity pattern of the derivative function. Moreover, because both the input and output to the algorithm are source codes, the algorithm may be applied recursively to generate derivatives of any order. A key component of the algorithm is its ability to statically exploit derivative sparsity at the MATLAB operation level to improve runtime performance. The algorithm is applied to four different classes of example problems and is shown to produce runtime efficient derivative code. Due to the static nature of the approach, the algorithm is well suited and intended for use with problems requiring many repeated derivative computations.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/TKL6L9EB/Weinstein and Rao - 2017 - Algorithm 984 ADiGator, a Toolbox for the Algorit.pdf}
}

@article{willkommNewUserInterface2014,
  title = {A new user interface for ADiMat: toward accurate and efficient derivatives of MATLAB programmes with ease of use},
  shorttitle = {A new user interface for ADiMat},
  author = {Willkomm, Johannes and Bischof, Christian H. and Bücker, H. Martin},
  year = {2014},
  month = jan,
  journal = {International Journal of Computational Science and Engineering},
  volume = {9},
  number = {5-6},
  pages = {408--415},
  publisher = {Inderscience Publishers},
  issn = {1742-7185},
  doi = {10.1504/IJCSE.2014.064526},
  url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJCSE.2014.064526},
  urldate = {2024-05-07},
  abstract = {Various techniques in computational science and engineering benefit from accurate and efficient derivative computation. ADiMat is a software tool that transforms a numerical program written in MATLAB into another MATLAB programme for the computation of derivatives that are free from truncation error. We introduce a new and easy-to-use interface for ADiMat and present case studies from geophysics and fluid mechanics to quantify the performance of the code generated by ADiMat.},
  keywords = {tracer}
}

@mastersthesis{xiongEfficientJacobianDetermination2014,
  title = {Efficient Jacobian Determination by Structure-Revealing Automatic Differentiation},
  author = {Xiong, Xin},
  year = {2014},
  month = jan,
  url = {https://uwspace.uwaterloo.ca/handle/10012/8197},
  urldate = {2024-05-07},
  abstract = {This thesis is concerned with the efficient computation of Jacobian matrices of nonlinear  vector maps using automatic differentiation (AD). Specifically, we propose the use of  two directed edge separator methods, the weighted minimum separator and natural order separator methods, to exploit the structure of the computational graph of the nonlinear system.This allows for the efficient determination of the Jacobian matrix using AD software. We will illustrate the promise of this approach with computational experiments.},
  langid = {english},
  school = {University of Waterloo},
  keywords = {tracer},
  annotation = {Accepted: 2014-01-23T21:14:25Z},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/XUSALP7I/Xiong - 2014 - Efficient Jacobian Determination by Structure-Reve.pdf}
}

@article{xuEfficientPartialDetermination2013,
  title = {Efficient (Partial) Determination of Derivative Matrices via Automatic Differentiation},
  author = {Xu, Wei and Coleman, Thomas F.},
  year = {2013},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {35},
  number = {3},
  pages = {A1398-A1416},
  publisher = {Society for Industrial and Applied Mathematics},
  issn = {1064-8275},
  doi = {10.1137/11085061X},
  url = {https://epubs.siam.org/doi/abs/10.1137/11085061X},
  urldate = {2024-05-07},
  abstract = {This paper is concerned with the efficient computation of sparse Jacobian matrices of nonlinear vector maps using automatic differentiation (AD). Specifically, we propose the use of a graph coloring technique, bicoloring, to exploit the sparsity of the Jacobian matrix J and thereby allow for the efficient determination of J using AD software. We analyze both a direct scheme and a substitution process. We discuss the results of numerical experiments indicating significant practical potential of this approach.},
  keywords = {tracer},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/F89UGNQW/Xu and Coleman - 2013 - Efficient (Partial) Determination of Derivative Ma.pdf}
}
